{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60022ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 模型搭建\n",
    "\n",
    "from datacleanv2 import *\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from collections import Counter # 用于统计计数的工具\n",
    "import time # 用于计时\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils # 用于处理变长序列，如填充和打包\n",
    "from torch.utils.data import Dataset, DataLoader # PyTorch 数据加载工具\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # 进度条库，使用 tqdm.tqdm\n",
    "import random\n",
    "import copy # 用于复制模型参数或列表\n",
    "import matplotlib.pyplot as plt # 用于绘图\n",
    "import seaborn as sns # 用于更美观的统计图，特别是热力图\n",
    "# 设置中文字体，解决可视化乱码问题\n",
    "plt.rcParams['font.family'] = ['SimHei'] # 使用黑体，或其他支持中文的字体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像时负号'-'显示为方块的问题\n",
    "\n",
    "\n",
    "# 设置运行设备，优先使用 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 超参数和常量定义\n",
    "NUM_MAIN_MODELS = 3 # 主要的聚类模型数量\n",
    "NUM_COMBINED_SETTINGS = 125 # combined_setting 的总类别数 (0-124)\n",
    "EMBEDDING_DIM = 16 # combined_setting 的嵌入向量维度，可调整\n",
    "HIDDEN_SIZE = 64   # RNN 隐藏层大小，可调整\n",
    "NUM_RNN_LAYERS = 2 # RNN 层数\n",
    "# 注意: TIME_LOSS_SCALER 可能需要根据实际 delta_t 的规模重新调整\n",
    "TIME_LOSS_SCALER = 1 # time delta_t MSE 损失的缩放因子，需要根据实际损失值大小调整\n",
    "TOTAL_EM_ITERATIONS = 10 # EM 迭代总次数 (根据要求修改为 10)\n",
    "CONVERGENCE_THRESHOLD = 0.05 # 收敛阈值，分配改变的序列比例低于此值时停止 (5%)\n",
    "\n",
    "# 干扰项处理参数\n",
    "NUM_RAND_SEQUENCES = 250 # 干扰项的已知数量\n",
    "INTERFERENCE_CLUSTER_LABEL = 3 # 将干扰项分配到的簇的索引 (0, 1, 2 是主簇，3 是干扰簇)\n",
    "INTERFERENCE_DETECTION_START_ITER = 2 # 从第 5 轮迭代 (索引 4) 的 E 步开始检测干扰项\n",
    "# 检测干扰项的高损失阈值：需要根据训练中观察到的损失值范围来调整\n",
    "# 如果一个序列在所有模型上的平均损失超过这个阈值，则可能被认为是干扰项。\n",
    "# ！！！重要参数，需要根据实际运行观察的损失值调整！！！\n",
    "# 在模拟数据上运行一次，观察损失值的分布，尤其是 rand_label 序列的损失。\n",
    "HIGH_AVG_LOSS_THRESHOLD = 0.5 ## <--- !!! 初始值，请务必根据实际情况调整 !!!\n",
    "\n",
    "# M 步训练参数 (每个 EM 迭代中的训练 epochs)\n",
    "# epochs 计划表：根据迭代次数使用不同数量的 epochs\n",
    "EPOCH_SCHEDULE = [1] * 5 + [2]* 5 # 示例：前 3 轮迭代训练 2 epoch，接下来 7 轮训练 5 epoch (适应总迭代 10)\n",
    "BATCH_SIZE = 32 # M 步训练时的批次大小\n",
    "# 在早期迭代中是否只使用部分批次来加速训练\n",
    "EARLY_ITER_BATCH_THRESHOLD = 3 # 在前 3 轮迭代中使用部分批次 (适应总迭代 10)\n",
    "EARLY_ITER_BATCH_PERCENT = 0.3 # 在启用部分批次训练时使用的批次比例 (30%)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. 定义 RNN 模型 SettingPredictorRNN (预测 delta_t 和 setting)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class SettingPredictorRNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size, num_rnn_layers, num_categories):\n",
    "        super(SettingPredictorRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_rnn_layers = num_rnn_layers\n",
    "        self.num_categories = num_categories\n",
    "\n",
    "        # 输入是 当前时间差 delta_t (1维) 和 combined_setting 的嵌入向量 (embedding_dim 维)\n",
    "        # 模型会根据当前时间差、设置和历史预测下一个时间步的时间差和设置\n",
    "        self.setting_embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "        input_size = 1 + embedding_dim\n",
    "\n",
    "        # 使用 GRU 作为 RNN 层\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_rnn_layers, batch_first=True)\n",
    "\n",
    "        # 输出层\n",
    "        # 预测下一个时间步的时间差 delta_t (回归问题，输出 1维)\n",
    "        self.time_delta_output = nn.Linear(hidden_size, 1)\n",
    "        # 预测下一个 combined_setting (分类问题，输出 num_categories 维的 logits)\n",
    "        self.setting_output = nn.Linear(hidden_size, num_categories)\n",
    "\n",
    "    # 前向传播，处理填充后的批次序列\n",
    "    def forward(self, time_delta_seq, setting_seq, lengths, hidden_state=None):\n",
    "        # time_delta_seq 形状: (batch_size, seq_len) - 填充后的 当前时间差 序列\n",
    "        # setting_seq 形状: (batch_size, seq_len) - 填充后的 当前 setting 整数索引序列 (long tensor)\n",
    "        # lengths: 原始输入序列长度的列表或 tensor (对应 time_delta_seq 和 setting_seq 的长度)\n",
    "\n",
    "        # 将 setting 整数索引转换为嵌入向量\n",
    "        setting_embedded = self.setting_embedding(setting_seq) # 形状: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # 组合当前时间差输入和嵌入后的 setting\n",
    "        input_seq = torch.cat((time_delta_seq.unsqueeze(-1), setting_embedded), dim=-1) # 形状: (batch_size, seq_len, 1 + embedding_dim)\n",
    "\n",
    "        # 打包填充后的序列\n",
    "        # lengths 必须在 CPU 上\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_seq, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # 通过 RNN 层\n",
    "        packed_output, hidden_state = self.rnn(packed_input, hidden_state)\n",
    "\n",
    "        # 将打包的序列重新填充回原始形状\n",
    "        output_seq, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True, total_length=input_seq.size(1)) # 形状: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # 通过输出层进行预测\n",
    "        # 预测的是下一个时间步的 delta_t (即输入序列中当前步对应的下一个 delta_t)\n",
    "        predicted_next_delta_t = self.time_delta_output(output_seq) # 形状: (batch_size, seq_len, 1)\n",
    "        # 预测的是下一个 setting 的 logits (即输入序列中当前步对应的下一个 setting)\n",
    "        predicted_next_setting_logits = self.setting_output(output_seq) # 形状: (batch_size, seq_len, num_categories)\n",
    "\n",
    "        # 压缩 predicted_next_delta_t 的最后一维\n",
    "        predicted_next_delta_t = predicted_next_delta_t.squeeze(-1) # 形状: (batch_size, seq_len)\n",
    "\n",
    "        return predicted_next_delta_t, predicted_next_setting_logits, hidden_state\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. 准备数据加载器 SequenceDataset 和 Collate Function (在 collate_fn 中计算 delta_t 输入和目标)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        自定义数据集类。\n",
    "        Args:\n",
    "            data_list: DataFrame 列表，每个 DataFrame 代表一个序列。\n",
    "                       DataFrame 应包含 'time' (float) 和\n",
    "                       'combined_setting' (类别类型，类别为 0-124 的整数) 列。\n",
    "                       注意：time 到 delta_t 的转换在 collate_fn 中完成。\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = self.data_list[idx]\n",
    "        # 将整个 'time' 和 'combined_setting' 序列返回\n",
    "        time_seq = torch.FloatTensor(df['time'].values)\n",
    "        setting_seq = torch.LongTensor(df['combined_setting'].astype(int).values) # 确保是 LongTensor 用于 embedding\n",
    "\n",
    "        # 返回完整序列数据和原始长度\n",
    "        return time_seq, setting_seq, len(df)\n",
    "\n",
    "# 批处理数据的 Collate Function\n",
    "def collate_fn(batch):\n",
    "    # batch 是一个元组列表：[(time_seq_1, setting_seq_1, len_1), ...]\n",
    "\n",
    "    # 根据原始序列长度降序排序批次\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # 解压批次数据\n",
    "    time_seqs_list, setting_seqs_list, original_lengths = zip(*batch)\n",
    "\n",
    "    # 过滤掉原始长度小于 3 的序列，这些序列无法构建有效的输入和目标序列 (长度 original_length - 2)\n",
    "    valid_indices = [i for i, length in enumerate(original_lengths) if length >= 3]\n",
    "\n",
    "    if not valid_indices:\n",
    "        # 如果批次中没有长度 >= 3 的序列，返回 None 表示空批次\n",
    "        return None\n",
    "\n",
    "    # 提取有效的序列和长度\n",
    "    time_seqs_list = [time_seqs_list[i] for i in valid_indices]\n",
    "    setting_seqs_list = [setting_seqs_list[i] for i in valid_indices]\n",
    "    valid_original_lengths = [original_lengths[i] for i in valid_indices]\n",
    "\n",
    "\n",
    "    # --- 计算输入序列 (当前 delta_t 和 setting) 和目标序列 (下一个 delta_t 和 setting) ---\n",
    "    # 它们都对应原始序列长度 - 2 的部分\n",
    "\n",
    "    # delta_t 输入: time[i+1] - time[i] for i from 0 to original_length - 3\n",
    "    delta_t_inputs_list = [(seq[1:-1] - seq[:-2]) for seq in time_seqs_list]\n",
    "    # setting 输入: setting[i] for i from 0 to original_length - 3\n",
    "    setting_inputs_list = [seq[:-2] for seq in setting_seqs_list]\n",
    "\n",
    "    # delta_t 目标: time[i+2] - time[i+1] for i from 0 to original_length - 3\n",
    "    delta_t_targets_list = [(seq[2:] - seq[1:-1]) for seq in time_seqs_list]\n",
    "    # setting 目标: setting[i+1] for i from 0 to original_length - 3\n",
    "    setting_targets_list = [seq[1:-1] for seq in setting_seqs_list]\n",
    "\n",
    "\n",
    "    # 输入/目标序列的长度都等于 original_length - 2\n",
    "    input_lengths = [length - 2 for length in valid_original_lengths]\n",
    "\n",
    "\n",
    "    # 填充输入序列\n",
    "    delta_t_inputs_padded = rnn_utils.pad_sequence(delta_t_inputs_list, batch_first=True, padding_value=0.0)\n",
    "    setting_inputs_padded = rnn_utils.pad_sequence(setting_inputs_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    # 填充目标序列\n",
    "    delta_t_targets_padded = rnn_utils.pad_sequence(delta_t_targets_list, batch_first=True, padding_value=0.0)\n",
    "    setting_targets_padded = rnn_utils.pad_sequence(setting_targets_list, batch_first=True, padding_value=0)\n",
    "\n",
    "\n",
    "    # 输入序列的有效长度\n",
    "    input_lengths_tensor = torch.LongTensor(input_lengths)\n",
    "\n",
    "    # 根据输入序列的有效长度重新排序批次\n",
    "    sorted_lengths, sorted_indices = torch.sort(input_lengths_tensor, descending=True)\n",
    "\n",
    "    # 按照 sorted_indices 对所有张量进行排序\n",
    "    delta_t_inputs_padded = delta_t_inputs_padded[sorted_indices]\n",
    "    setting_inputs_padded = setting_inputs_padded[sorted_indices]\n",
    "    delta_t_targets_padded = delta_t_targets_padded[sorted_indices]\n",
    "    setting_targets_padded = setting_targets_padded[sorted_indices]\n",
    "\n",
    "    # 将张量移动到设备\n",
    "    delta_t_inputs_padded = delta_t_inputs_padded.to(device)\n",
    "    setting_inputs_padded = setting_inputs_padded.to(device)\n",
    "    delta_t_targets_padded = delta_t_targets_padded.to(device)\n",
    "    setting_targets_padded = setting_targets_padded.to(device)\n",
    "    # sorted_lengths 保持在 CPU\n",
    "\n",
    "    # 返回 delta_t 输入, setting 输入, delta_t 目标, setting 目标, 输入长度\n",
    "    return delta_t_inputs_padded, setting_inputs_padded, delta_t_targets_padded, setting_targets_padded, sorted_lengths\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. 实现训练函数 train_model (使用 delta_t 目标计算时间预测损失)\n",
    "#    接收模型索引作为参数进行打印。\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def train_model(model, model_idx, dataloader, optimizer, time_criterion, setting_criterion, epochs, time_scaler, iteration_num):\n",
    "    model.train() # 设置模型为训练模式\n",
    "    total_batches = len(dataloader)\n",
    "    # 根据迭代次数决定使用的批次数量\n",
    "    if iteration_num < EARLY_ITER_BATCH_THRESHOLD:\n",
    "        batches_to_use = max(1, int(total_batches * EARLY_ITER_BATCH_PERCENT))\n",
    "    else:\n",
    "        batches_to_use = total_batches\n",
    "\n",
    "    # 使用 tqdm 显示 epoch 进度，使用传入的 model_idx\n",
    "    epoch_tqdm = tqdm(range(epochs), desc=f\"迭代 {iteration_num} (模型 {model_idx}) 训练\", leave=False)\n",
    "    for epoch in epoch_tqdm:\n",
    "        batch_count = 0\n",
    "        # 使用 tqdm 显示批次进度\n",
    "        batch_tqdm = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch_data in batch_tqdm:\n",
    "            if batch_data is None: continue # 跳过空批次\n",
    "\n",
    "            # 从 collate_fn 获取批次数据\n",
    "            # 获取的是 delta_t 输入\n",
    "            delta_t_inputs, setting_inputs, delta_t_targets, setting_targets, lengths = batch_data\n",
    "\n",
    "            optimizer.zero_grad() # 清零梯度\n",
    "\n",
    "            # 前向传播\n",
    "            # 模型现在输出的是 predicted_next_delta_t 和 predicted_next_setting_logits\n",
    "            predicted_next_delta_t, predicted_next_setting_logits, _ = model(delta_t_inputs, setting_inputs, lengths)\n",
    "\n",
    "            # 计算损失\n",
    "            # predicted_next_delta_t 形状: (batch_size, seq_len)\n",
    "            # delta_t_targets 形状: (batch_size, seq_len)\n",
    "            time_loss = time_criterion(predicted_next_delta_t, delta_t_targets)\n",
    "\n",
    "            # predicted_next_setting_logits 形状: (batch_size, seq_len, num_categories)\n",
    "            # setting_targets 形状: (batch_size, seq_len)\n",
    "            # 需要调整 logits 的维度到 (batch_size, num_categories, seq_len)\n",
    "            setting_loss = setting_criterion(predicted_next_setting_logits.permute(0, 2, 1), setting_targets)\n",
    "\n",
    "            # 计算总损失，并应用时间损失缩放因子\n",
    "            loss = time_loss * time_scaler + setting_loss\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            # 可选：梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_count += 1\n",
    "            # 在早期迭代中，只训练部分批次\n",
    "            if batch_count >= batches_to_use:\n",
    "                 break\n",
    "\n",
    "            # 更新批次进度条的后缀信息（可选）\n",
    "            batch_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        # 更新 epoch 进度条的后缀信息（可选）\n",
    "        # 注意：这里显示的是最后一个批次的损失，不是平均损失\n",
    "        epoch_tqdm.set_postfix(last_batch_loss=loss.item())\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. 实现评估函数 evaluate_sequence_loss (计算单个序列 **平均** 损失)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_sequence_loss(model, time_seq_full, setting_seq_full, time_criterion, setting_criterion, time_scaler):\n",
    "    model.eval() # 设置模型为评估模式\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "\n",
    "        seq_len = time_seq_full.size(0)\n",
    "        # 序列长度小于 3 无法构建输入和目标序列 (长度 original_length - 2)\n",
    "        if seq_len < 3:\n",
    "            # 返回无穷大，表示无法计算有效损失，在比较时会被排除\n",
    "            return float('inf')\n",
    "\n",
    "        # --- 构建输入序列 (当前 delta_t 和 setting) 和目标序列 (下一个 delta_t 和 setting) ---\n",
    "        # 它们都对应原始序列长度 - 2 的部分\n",
    "\n",
    "        # delta_t 输入: time[i+1] - time[i] for i from 0 to seq_len - 3\n",
    "        delta_t_inputs_sliced = (time_seq_full[1:-1] - time_seq_full[:-2]).unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "        # setting 输入: setting[i] for i from 0 to seq_len - 3\n",
    "        setting_inputs_sliced = setting_seq_full[:-2].unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "\n",
    "        # delta_t 目标: time[i+2] - time[i+1] for i from 0 to seq_len - 3\n",
    "        delta_t_targets_sliced = (time_seq_full[2:] - time_seq_full[1:-1]).unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "        # setting 目标: setting[i+1] for i from 0 to seq_len - 3\n",
    "        setting_targets_sliced = setting_seq_full[1:-1].unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "\n",
    "        # 输入序列的实际长度\n",
    "        eval_input_len = seq_len - 2 # 有效的预测步数\n",
    "        lengths = torch.tensor([eval_input_len]) # 保持在 CPU\n",
    "\n",
    "        # 前向传播，计算整个序列的预测结果 (长度为 eval_input_len)\n",
    "        predicted_next_delta_t, predicted_next_setting_logits, _ = model(delta_t_inputs_sliced, setting_inputs_sliced, lengths)\n",
    "\n",
    "        # 计算整个序列的损失 (注意：损失函数如 MSELoss 和 CrossEntropyLoss 默认计算的是批次和序列长度上的平均)\n",
    "        # 但在这里我们处理的是单个序列 (batch_size = 1)，并且使用了 pack_padded_sequence，\n",
    "        # PyTorch 会确保损失计算只在有效长度上进行。\n",
    "        # 所以 time_criterion(predicted_next_delta_t, delta_t_targets_sliced) 计算的是 batch 和 有效长度上的平均损失。\n",
    "        # setting_criterion(...) 也类似。\n",
    "\n",
    "        time_loss = time_criterion(predicted_next_delta_t, delta_t_targets_sliced)\n",
    "        setting_loss = setting_criterion(predicted_next_setting_logits.permute(0, 2, 1), setting_targets_sliced)\n",
    "\n",
    "        # 计算总损失 (按步加权平均)\n",
    "        total_loss_per_step = time_loss * time_scaler + setting_loss\n",
    "\n",
    "        # total_loss_per_step 现在已经是每个预测步的平均损失了 (因为 MSELoss/CrossEntropyLoss 默认对 batch 和序列长度取平均)\n",
    "        # 但是 pack_padded_sequence 的行为可能会影响这个平均，为了安全和明确，我们还是按总损失再除以步数\n",
    "        # 更稳妥的方法是使用 reduction='sum' 然后手动除以有效步数\n",
    "        time_criterion_sum = nn.MSELoss(reduction='sum')\n",
    "        setting_criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        time_loss_sum = time_criterion_sum(predicted_next_delta_t, delta_t_targets_sliced)\n",
    "        setting_loss_sum = setting_criterion_sum(predicted_next_setting_logits.permute(0, 2, 1), setting_targets_sliced)\n",
    "\n",
    "        total_sum_loss = time_loss_sum * time_scaler + setting_loss_sum\n",
    "    \n",
    "        # 计算 **平均** 损失：总和损失除以有效预测步数\n",
    "        average_loss_per_step = total_sum_loss / eval_input_len\n",
    "        average_set_loss_per_step = setting_loss_sum / eval_input_len\n",
    "\n",
    "\n",
    "        # 返回标量平均损失值\n",
    "        return average_loss_per_step.item(),average_set_loss_per_step.item()\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. 实现 EM-like 聚类算法 run_rnn_clustering (加入干扰项处理)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def run_rnn_clustering(transformed_list, num_main_models, embedding_dim, hidden_size, num_rnn_layers,\n",
    "                       num_categories, time_scaler, total_iterations, convergence_threshold,\n",
    "                       epoch_schedule, batch_size, early_iter_batch_threshold, early_iter_batch_percent,\n",
    "                       interference_cluster_label, interference_detection_start_iter, high_avg_loss_threshold, num_rand_sequences):\n",
    "\n",
    "    total_sequences = len(transformed_list)\n",
    "    print(f\"开始基于 RNN 的聚类，共有 {total_sequences} 条序列，聚成 {num_main_models} 类 + 1 干扰类 ({interference_cluster_label})。\")\n",
    "\n",
    "    # 1. 初始化\n",
    "    # 创建 NUM_MAIN_MODELS 个结构相同的 RNN 模型\n",
    "    models = [SettingPredictorRNN(embedding_dim, hidden_size, num_rnn_layers, num_categories).to(device) for _ in range(num_main_models)]\n",
    "\n",
    "    # current_assignments: 存储当前迭代中用于 M 步训练和 E 步重新分配的簇分配 (0, 1, 2, 或 interference_cluster_label)。\n",
    "    # 初始化时，所有序列随机分配到主簇。长度不足 3 的序列暂时分配到 -1。\n",
    "    current_assignments = np.full(total_sequences, -1, dtype=int) # 初始化为 -1\n",
    "    # 先将长度 >= 3 的序列随机分配到主簇\n",
    "    long_enough_indices = np.where(np.array([len(item[0]) for item in transformed_list]) >= 3)[0]\n",
    "    current_assignments[long_enough_indices] = np.random.randint(0, num_main_models, len(long_enough_indices))\n",
    "\n",
    "\n",
    "    print(f\"初始随机分配到主簇 (长度>=3 的序列): {np.bincount(current_assignments[current_assignments != -1], minlength=num_main_models)}\")\n",
    "    print(f\"初始未分配序列 (长度<3): {np.sum(current_assignments == -1)}\")\n",
    "\n",
    "\n",
    "    # 定义损失函数 (用于 E 步计算单序列损失，使用 reduction='sum')\n",
    "    time_criterion_sum = nn.MSELoss(reduction='sum')\n",
    "    setting_criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "    # 定义损失函数 (用于 M 步训练批次，使用默认 reduction='mean')\n",
    "    time_criterion_mean = nn.MSELoss()\n",
    "    setting_criterion_mean = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    # 记录被标记为干扰项的序列的原始索引 (仅用于跟踪和最终返回)\n",
    "    removed_interference_indices_tracker = []\n",
    "\n",
    "    # 2. EM-like 迭代循环\n",
    "    main_tqdm = tqdm(range(total_iterations), desc=\"EM 迭代总进度\")\n",
    "    for iter_num in main_tqdm:\n",
    "        main_tqdm.set_description(f\"EM 迭代 {iter_num + 1}/{total_iterations}\")\n",
    "\n",
    "        # prev_assignments: 记录本轮迭代 E 步开始前 current_assignments 的状态，用于收敛检查 (仅检查主簇变化)\n",
    "        prev_assignments = current_assignments.copy()\n",
    "\n",
    "        # active_indices: 获取当前仍在参与聚类的主簇序列的原始索引 (即 current_assignments != interference_cluster_label 的序列)\n",
    "        active_indices_mask = (current_assignments != interference_cluster_label) & (current_assignments != -1) # 排除已标记为干扰项和尚未初始分配的 (-1)\n",
    "        active_indices = np.where(active_indices_mask)[0]\n",
    "        current_total_active_sequences = len(active_indices)\n",
    "\n",
    "\n",
    "        # 只有当还有活跃序列时才进行 M 步训练\n",
    "        if current_total_active_sequences > 0:\n",
    "             print(f\"\\n--- 迭代 {iter_num + 1}: M 步 (训练模型)，当前活跃序列数: {current_total_active_sequences} ---\")\n",
    "             # 获取本轮迭代应训练的 epochs 数量\n",
    "             current_epochs = epoch_schedule[min(iter_num, len(epoch_schedule) - 1)]\n",
    "\n",
    "             # M 步: 根据 current_assignments 训练每个模型 (仅使用活跃序列)\n",
    "             for model_idx in range(num_main_models):\n",
    "                 # 找到当前分配给该模型的所有 *活跃* 序列的原始索引\n",
    "                 assigned_indices_in_active = active_indices[current_assignments[active_indices] == model_idx]\n",
    "\n",
    "                 if len(assigned_indices_in_active) == 0:\n",
    "                     print(f\"  模型 {model_idx} 没有活跃序列分配到，跳过训练。\")\n",
    "                     continue\n",
    "\n",
    "                 # 获取这些索引对应的原始数据框\n",
    "                 # Dataset 会只加载这些索引对应的 df\n",
    "                 assigned_dataset = SequenceDataset([transformed_list[i][0] for i in assigned_indices_in_active])\n",
    "                 # collate_fn 将过滤掉长度不足 3 的序列，DataLoader 会处理批次和填充\n",
    "                 assigned_dataloader = DataLoader(assigned_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "                 # 只有当 DataLoader 不为空时才训练 (即存在长度 >= 3 的序列)\n",
    "                 if len(assigned_dataloader) > 0:\n",
    "                    # 为当前模型创建一个优化器\n",
    "                    optimizer = optim.Adam(models[model_idx].parameters())\n",
    "\n",
    "                    # 调用训练函数训练当前模型，传入模型索引，使用 mean reduction 的损失函数\n",
    "                    train_model(models[model_idx], model_idx, assigned_dataloader, optimizer,\n",
    "                                time_criterion_mean, setting_criterion_mean, current_epochs,\n",
    "                                time_scaler, iter_num)\n",
    "                 else:\n",
    "                     print(f\"  模型 {model_idx} 分配到的活跃序列中没有长度 >= 3 的，跳过训练。\")\n",
    "\n",
    "        else:\n",
    "            print(f\"\\n--- 迭代 {iter_num + 1}: M 步 (训练模型)，当前没有活跃序列，跳过训练。---\")\n",
    "            # 如果没有活跃序列，且不是第一轮，可能已经收敛或所有都被标记为干扰项\n",
    "            if iter_num > 0:\n",
    "                 print(\"\\n没有活跃序列，且不是第一轮迭代，可能已收敛。\")\n",
    "                 break # 跳出循环\n",
    "\n",
    "\n",
    "        print(f\"\\n--- 迭代 {iter_num + 1}: E 步 (重分配序列及干扰项检测) ---\")\n",
    "\n",
    "        # 在 E 步开始时，找出所有未被最终标记为干扰项的序列 (可能包含长度不足 3 的)\n",
    "        # 这些是 candidates for re-assignment or interference detection\n",
    "        candidate_indices_mask = (current_assignments != interference_cluster_label) # 只要不是干扰项，都是候选\n",
    "        candidate_indices = np.where(candidate_indices_mask)[0]\n",
    "        current_total_candidates = len(candidate_indices)\n",
    "\n",
    "        if current_total_candidates == 0:\n",
    "             print(\"  当前没有候选序列需要评估 (所有序列都已标记为干扰项)。\")\n",
    "             break # 跳出循环\n",
    "\n",
    "        # 计算所有 *候选* 序列在所有主模型上的损失 (使用 sum reduction 的损失函数)\n",
    "        # losses_for_candidates 形状: (当前候选序列总数, 主模型总数)\n",
    "        losses_for_candidates = np.full((current_total_candidates, num_main_models), float('inf')) # 初始化为无穷大\n",
    "        set_losses_for_candidates = np.full((current_total_candidates, num_main_models), float('inf'))\n",
    "        print(f\"  计算 {current_total_candidates} 条候选序列在每个模型上的平均损失...\")\n",
    "        eval_tqdm = tqdm(range(current_total_candidates), desc=\"计算损失\", leave=False)\n",
    "        for j in eval_tqdm:\n",
    "             original_idx = candidate_indices[j] # 获取对应的原始索引\n",
    "             df = transformed_list[original_idx][0] # 获取数据框\n",
    "\n",
    "             time_seq_full = torch.FloatTensor(df['time'].values)\n",
    "             setting_seq_full = torch.LongTensor(df['combined_setting'].astype(int).values)\n",
    "\n",
    "             # evaluate_sequence_loss 内部会检查长度是否 >= 3，并返回 inf 或平均损失\n",
    "             for model_idx in range(num_main_models):\n",
    "                 # evaluate_sequence_loss 现在返回的是平均损失\n",
    "                 avg_loss,set_loss = evaluate_sequence_loss(models[model_idx], time_seq_full, setting_seq_full,\n",
    "                                                   time_criterion_sum, setting_criterion_sum, time_scaler)\n",
    "                 losses_for_candidates[j, model_idx],set_losses_for_candidates[j, model_idx] = avg_loss,set_loss # 记录平均损失\n",
    "\n",
    "        # --- 干扰项检测和移除 ---\n",
    "        # 在指定迭代轮次之后进行干扰项检测\n",
    "        if iter_num >= interference_detection_start_iter -1 :\n",
    "            print(\"  进行干扰项检测...\")\n",
    "            # 只考虑那些所有模型损失都不是无穷大的候选序列 (即长度 >= 3 的序列)\n",
    "            valid_loss_candidate_indices_relative = np.where(np.isfinite(losses_for_candidates).all(axis=1))[0]\n",
    "            # 对应的原始索引\n",
    "            valid_loss_candidates_original_indices = candidate_indices[valid_loss_candidate_indices_relative]\n",
    "            # 对应的平均损失矩阵 (只包含长度 >= 3 的序列)\n",
    "            valid_avg_losses = losses_for_candidates[valid_loss_candidate_indices_relative]\n",
    "            set_avg_losses = set_losses_for_candidates[valid_loss_candidate_indices_relative]\n",
    "\n",
    "            if len(valid_loss_candidate_indices_relative) > 0:\n",
    "                 # 计算这些序列在所有主模型上的 平均平均损失 (只是为了排序，不是必须的)\n",
    "                 min_of_avg_losses_valid_candidates = valid_avg_losses.var(axis=1)#改：应该计算所有主模型中的最低损失而非平均损失\n",
    "\n",
    "                 # 识别 平均损失 的 平均值 高于阈值的有效损失候选序列的索引 (在 valid_loss_candidate_indices_relative 中的相对索引)\n",
    "                 high_avg_loss_valid_candidate_indices_relative_to_valid = np.where(min_of_avg_losses_valid_candidates > high_avg_loss_threshold)[0]\n",
    "\n",
    "                 # 将这些高平均损失有效候选序列的相对索引映射回原始索引\n",
    "                 high_avg_loss_candidates_original_indices = valid_loss_candidates_original_indices[high_avg_loss_valid_candidate_indices_relative_to_valid]\n",
    "\n",
    "\n",
    "                 # 从高平均损失候选者中，选取平均损失最高的序列作为干扰项\n",
    "                 # 确保不超过已知干扰项总数 (num_rand_sequences)，并且不重复移除\n",
    "                 # 找到尚未被标记为干扰项的高平均损失候选者\n",
    "                 currently_not_removed_high_avg_loss_mask = np.isin(high_avg_loss_candidates_original_indices, removed_interference_indices_tracker, invert=True)\n",
    "                 currently_not_removed_high_avg_loss_original_indices = high_avg_loss_candidates_original_indices[currently_not_removed_high_avg_loss_mask]\n",
    "\n",
    "                 # 对这些尚未被移除的高平均损失序列，按平均损失的平均值降序排序\n",
    "                 # 需要获取这些序列在 min_of_avg_losses_valid_candidates 中的对应值\n",
    "                 # 找到 currently_not_removed_high_avg_loss_original_indices 在 valid_loss_candidates_original_indices 中的相对索引\n",
    "                 relative_indices_in_valid_for_candidates_to_sort = np.where(np.isin(valid_loss_candidates_original_indices, currently_not_removed_high_avg_loss_original_indices))[0]\n",
    "                 min_of_avg_losses_for_sorting = min_of_avg_losses_valid_candidates[relative_indices_in_valid_for_candidates_to_sort]\n",
    "                 # 对原始索引进行排序，基于其对应的平均损失的平均值\n",
    "                 #sorted_candidates_original_indices = currently_not_removed_high_avg_loss_original_indices[np.argsort(min_of_avg_losses_for_sorting)[::-1]]\n",
    "                 sorted_candidates_original_indices = currently_not_removed_high_avg_loss_original_indices[np.argsort(min_of_avg_losses_for_sorting) ]\n",
    "\n",
    "             \n",
    "                 # 选取前 N 个作为本轮被标记为干扰项的序列\n",
    "                 num_to_select_this_iter = min(len(sorted_candidates_original_indices), num_rand_sequences - len(removed_interference_indices_tracker),20)\n",
    "                 indices_to_remove_this_iter_original = sorted_candidates_original_indices[:num_to_select_this_iter]#每轮最多标记20个\n",
    "\n",
    "                 # 将这些序列标记为干扰项 (在 current_assignments 中设置为干扰簇标签)\n",
    "                 current_assignments[indices_to_remove_this_iter_original] = interference_cluster_label\n",
    "                 # 添加到已移除列表中 (跟踪用)\n",
    "                 removed_interference_indices_tracker.extend(indices_to_remove_this_iter_original)\n",
    "                 removed_count_this_iter = len(indices_to_remove_this_iter_original)\n",
    "                 print(f\"  本轮标记 {removed_count_this_iter} 条序列为干扰项。总计已标记 {len(removed_interference_indices_tracker)} 条。\")\n",
    "            else:\n",
    "                 print(\"  没有符合高平均损失阈值的候选序列 (长度 >= 3)，本轮未标记新的干扰项。\")\n",
    "        else:\n",
    "            print(\"  干扰项检测尚未开始。\")\n",
    "\n",
    "\n",
    "        # --- 主簇重新分配 ---\n",
    "        # 找到当前仍未被标记为干扰项的序列 (这些是需要重新分配到主簇或保持主簇分配的序列)\n",
    "        remaining_main_cluster_candidate_indices_mask = (current_assignments != interference_cluster_label) # 修复：只要不是干扰项，都是候选\n",
    "        remaining_main_cluster_candidate_indices = np.where(remaining_main_cluster_candidate_indices_mask)[0]\n",
    "\n",
    "\n",
    "        # 从 losses_for_candidates 中，提取出这些剩余主簇候选序列的损失\n",
    "        # losses_for_candidates 的行是按照 E 步开始时的 candidate_indices 的顺序排列的\n",
    "        # 找到 remaining_main_cluster_candidate_indices 在 E 步开始时的 candidate_indices 中的位置 (相对索引)\n",
    "        relative_indices_in_candidates_for_remaining = np.where(np.isin(candidate_indices, remaining_main_cluster_candidate_indices))[0]\n",
    "        losses_for_remaining_main_candidates = losses_for_candidates[relative_indices_in_candidates_for_remaining]\n",
    "\n",
    "        # 只对那些有有效平均损失的序列 (长度 >= 3) 进行重新分配 (排除 inf)\n",
    "        valid_loss_remaining_mask = np.isfinite(losses_for_remaining_main_candidates).all(axis=1)\n",
    "        valid_loss_remaining_relative_indices_in_candidates = relative_indices_in_candidates_for_remaining[valid_loss_remaining_mask]\n",
    "        valid_loss_remaining_original_indices = candidate_indices[valid_loss_remaining_relative_indices_in_candidates] # 这些是需要重新分配的序列的原始索引\n",
    "\n",
    "\n",
    "        if len(valid_loss_remaining_original_indices) > 0:\n",
    "             # 找到这些序列在三个主模型上的 **平均损失** 最小的模型索引 (0, 1, 或 2)\n",
    "             losses_for_valid_remaining = losses_for_candidates[valid_loss_remaining_relative_indices_in_candidates]\n",
    "             new_main_assignments_for_valid_remaining = losses_for_valid_remaining.argmin(axis=1)\n",
    "\n",
    "             # 更新这些序列在 current_assignments 中的分组 (设置为 0, 1, 或 2)\n",
    "             current_assignments[valid_loss_remaining_original_indices] = new_main_assignments_for_valid_remaining\n",
    "\n",
    "             print(f\"  {len(valid_loss_remaining_original_indices)} 条长度 >= 3 的序列被重新分配到主簇。\")\n",
    "\n",
    "        else:\n",
    "             print(\"  没有长度 >= 3 的序列需要重新分配到主簇。\")\n",
    "\n",
    "\n",
    "        # 检查收敛性：比较本轮迭代 E 步开始前和结束后的 current_assignments (仅考虑主簇)\n",
    "        # 找到在两轮迭代中都未被标记为干扰项的序列的原始索引\n",
    "        stable_indices_mask = (prev_assignments != interference_cluster_label) & (current_assignments != interference_cluster_label) & (prev_assignments != -1) & (current_assignments != -1) # 修复：排除-1的序列\n",
    "        stable_indices = np.where(stable_indices_mask)[0]\n",
    "\n",
    "        if len(stable_indices) > 0:\n",
    "            # 比较这些稳定序列在两轮迭代中的主簇分配\n",
    "            prev_main_assignments_stable = prev_assignments[stable_indices]\n",
    "            current_main_assignments_stable = current_assignments[stable_indices]\n",
    "\n",
    "            # 计算改变主簇分配的序列数量 (仅在稳定序列中)\n",
    "            num_changes = np.sum(prev_main_assignments_stable != current_main_assignments_stable)\n",
    "            # 计算改变分组的序列比例 (基于总序列数)\n",
    "            change_percent = num_changes / total_sequences\n",
    "\n",
    "            print(f\"  未被标记干扰项且已分配到主簇的序列中，有 {num_changes} 条改变了主簇 ({change_percent:.2%})。\")\n",
    "\n",
    "            # 判断是否收敛：如果改变比例低于阈值，且不是第一轮迭代 (iter_num > 0)\n",
    "            if change_percent < convergence_threshold and iter_num > 0:\n",
    "                print(f\"收敛达成。总分组改变比例 ({change_percent:.2%}) 低于阈值 ({convergence_threshold:.2%})。\")\n",
    "                break\n",
    "        elif iter_num > 0: # 如果没有稳定序列，且不是第一轮\n",
    "             print(\"  没有稳定序列 (未被标记干扰项且已分配到主簇)，无法计算收敛率。\")\n",
    "             pass # 继续迭代或添加其他条件\n",
    "\n",
    "        else: # 第一轮迭代结束，没有上一轮状态进行比较\n",
    "             print(\"  第一轮迭代结束，无法计算收敛率。\")\n",
    "\n",
    "\n",
    "        # 打印当前的分组统计 (包含干扰项和未分配的 -1)\n",
    "        print(f\"  当前分组统计: {np.bincount(current_assignments + 1, minlength=num_main_models + 2)}\") # +1 让 -1 变为 0，0 变为 1 等，minlength 确保包含所有可能的类别\n",
    "        print(f\"    (-1: {np.sum(current_assignments == -1)}, 0-{num_main_models-1}: {np.bincount(current_assignments[current_assignments >= 0], minlength=num_main_models)[:num_main_models]}, {interference_cluster_label}: {np.sum(current_assignments == interference_cluster_label)})\")\n",
    "\n",
    "\n",
    "        # 如果达到最大迭代次数\n",
    "        if iter_num == total_iterations - 1:\n",
    "             print(\"达到最大迭代次数。\")\n",
    "\n",
    "        main_tqdm.update(0) # 更新主进度条显示的信息 (主要是当前迭代数)\n",
    "\n",
    "    # 迭代结束\n",
    "    # 将最终的 current_assignments 赋值给 final_assignments\n",
    "    final_assignments = current_assignments.copy()\n",
    "\n",
    "    # 将剩余未分配的序列 (-1) 也标记为干扰项\n",
    "    remaining_unassigned_indices = np.where(final_assignments == -1)[0]\n",
    "    if len(remaining_unassigned_indices) > 0:\n",
    "        print(f\"迭代结束，将剩余 {len(remaining_unassigned_indices)} 条未分配序列标记为干扰项。\")\n",
    "        final_assignments[remaining_unassigned_indices] = interference_cluster_label\n",
    "        # 添加到最终跟踪列表中\n",
    "        # 修复：只添加尚未被标记为干扰项的未分配序列\n",
    "        already_removed_mask = np.isin(remaining_unassigned_indices, removed_interference_indices_tracker)\n",
    "        newly_removed_indices = remaining_unassigned_indices[~already_removed_mask]\n",
    "        removed_interference_indices_tracker.extend(newly_removed_indices)\n",
    "\n",
    "\n",
    "    print(\"\\n--- 聚类完成 ---\")\n",
    "    # 最终分组统计 (包含干扰项)\n",
    "    assigned_indices_mask = final_assignments != -1\n",
    "    if assigned_indices_mask.sum() > 0:\n",
    "        final_clusters_counts = np.bincount(final_assignments[assigned_indices_mask], minlength=num_main_models + 1)\n",
    "        print(f\"最终各簇包含的序列数量 (0-{num_main_models-1} 是主簇, {interference_cluster_label} 是干扰簇):\")\n",
    "        print(f\"簇 0-{num_main_models-1}: {final_clusters_counts[:num_main_models]}, 干扰项 ({interference_cluster_label}): {final_clusters_counts[interference_cluster_label] if interference_cluster_label < len(final_clusters_counts) else 0}\")\n",
    "    else:\n",
    "        print(\"最终所有序列都未被分配到任何簇。\")\n",
    "\n",
    "\n",
    "    return models, final_assignments, removed_interference_indices_tracker # 返回 models\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6. 聚类结果可视化函数 (包含干扰项类别)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def visualize_clustering_results(transformed_list, final_assignments, num_main_models, interference_cluster_label):\n",
    "    \"\"\"\n",
    "    可视化聚类结果，显示原始标签与分配到的簇之间的交叉关系 (包含干扰项类别)。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 聚类结果可视化 ---\")\n",
    "\n",
    "    original_labels = [item[1] for item in transformed_list] # 提取所有原始标签\n",
    "    # 根据最终分组创建分组标签 (0, 1, 2 对应主簇，干扰簇使用单独标签)\n",
    "    assignment_labels = [f\"簇_{a}\" if a != interference_cluster_label else \"干扰项\" for a in final_assignments]\n",
    "\n",
    "    # 创建原始标签和分配簇的 DataFrame\n",
    "    results_df = pd.DataFrame({'原始标签': original_labels, '分配到的簇': assignment_labels})\n",
    "\n",
    "    # 确保交叉表中包含所有可能的原始标签和所有可能的分配簇标签\n",
    "    all_original_labels = sorted(list(set(original_labels))) # 包含 onestep, repeat, reset, rand\n",
    "    all_assignment_labels = [f\"簇_{i}\" for i in range(num_main_models)] + [\"干扰项\"]\n",
    "    # 确保 '干扰项' 标签在分配簇中存在，即使没有序列被分到，以便reindex不出错\n",
    "    if \"干扰项\" not in all_assignment_labels:\n",
    "         all_assignment_labels.append(\"干扰项\")\n",
    "    # 按照簇号排序主簇标签，干扰项放最后\n",
    "    all_assignment_labels = sorted(all_assignment_labels, key=lambda x: int(x.split('_')[1]) if x.startswith('簇_') else 999)\n",
    "\n",
    "\n",
    "    # 计算交叉表\n",
    "    # .reindex 确保行和列按照指定的顺序和名称显示，没有的填 0\n",
    "    crosstab_df = pd.crosstab(results_df['原始标签'], results_df['分配到的簇']).reindex(index=all_original_labels, columns=all_assignment_labels, fill_value=0)\n",
    "\n",
    "    print(\"\\n原始标签与分配到的簇的交叉表:\")\n",
    "    print(crosstab_df)\n",
    "\n",
    "    # 绘制热力图\n",
    "    plt.figure(figsize=(10, 7)) # 调整图大小以容纳更多标签\n",
    "    sns.heatmap(crosstab_df, annot=True, fmt='d', cmap='Blues', linewidths=.5)\n",
    "    plt.title('原始标签与分配到的簇的交叉表', fontsize=14)\n",
    "    plt.xlabel('分配到的簇', fontsize=12)\n",
    "    plt.ylabel('原始标签', fontsize=12)\n",
    "    plt.tight_layout() # 自动调整布局，防止标签重叠\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7. 单样本步进式预测函数及可视化 (仅针对非干扰项)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def predict_sequence_step_by_step(model, dataframe, num_categories):\n",
    "    \"\"\"\n",
    "    使用模型对单个序列进行步进式预测，并返回实际值和预测值。\n",
    "    预测的输入是当前 delta_t 和 setting，预测下一个 delta_t 和 setting。\n",
    "    实际预测时间需要根据实际时间进行累加。\n",
    "\n",
    "    Args:\n",
    "        model: 训练好的 RNN 模型。\n",
    "        dataframe: 单个序列的 DataFrame，包含 'time' (绝对时间) 和 'combined_setting' 列。\n",
    "        num_categories: setting 的总类别数 (125)。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (actual_time_sliced, predicted_time_abs, actual_setting_sliced, predicted_setting)\n",
    "               actual_time_sliced: 实际的时间序列 (从原始序列索引 1 开始) (numpy array)\n",
    "               predicted_time_abs: 预测的绝对时间序列 (从原始序列索引 1 开始) (numpy array)\n",
    "               actual_setting_sliced: 实际的设置序列 (从原始序列索引 1 开始) (numpy array, 整数 0-124)\n",
    "               predicted_setting: 预测的设置序列 (从原始序列索引 1 开始) (numpy array, 整数 0-124)\n",
    "               注意：预测序列和切片的实际序列长度是 seq_len - 1。\n",
    "               如果无法预测 (序列太短)，返回 None, None, None, None。\n",
    "    \"\"\"\n",
    "    model.eval() # 设置模型为评估模式\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "\n",
    "        # 确保 dataframe 长度足够进行预测 (至少需要 2 步才能计算第一个 delta_t 并预测下一步)\n",
    "        seq_len = len(dataframe)\n",
    "        if seq_len < 2: # 修复：预测一步需要至少 2个点\n",
    "            return None, None, None, None\n",
    "\n",
    "        # 提取实际序列值\n",
    "        actual_time = dataframe['time'].values.astype(np.float32)\n",
    "        actual_setting = dataframe['combined_setting'].astype(int).values.astype(np.int64) # 确保是整数类型\n",
    "\n",
    "        # 初始化存储预测结果的列表\n",
    "        predicted_delta_t_list = []\n",
    "        predicted_setting_list = []\n",
    "\n",
    "        # 初始化 RNN 的隐藏状态\n",
    "        hidden_state = None # GRU 的隐藏状态可以是 None\n",
    "\n",
    "        # 步进式预测循环：\n",
    "        # 循环遍历实际序列，从第一个时间步 (索引 i=0) 作为模型的输入起点\n",
    "        # 输入到模型的是 actual_time[i+1]-actual_time[i] (delta_t 输入) 和 actual_setting[i] (setting 输入)\n",
    "        # 预测的是 actual_time[i+2]-actual_time[i+1] (预测 delta_t) 和 actual_setting[i+1] (预测 setting)\n",
    "        # 循环范围是 i 从 0 到 seq_len - 2 (共 seq_len - 1 步预测)\n",
    "        # 第 i 步的输入：actual_time[i+1]-actual_time[i] 和 actual_setting[i]\n",
    "        # 预测输出对应实际序列的索引 i+1 的时间差和设置\n",
    "\n",
    "        # 需要实际序列的前 seq_len - 1 个 delta_t 作为模型的输入序列\n",
    "        actual_delta_t_inputs_for_rnn = actual_time[1:] - actual_time[:-1] # 形状: (seq_len - 1)\n",
    "        # 需要实际序列的前 seq_len - 1 个 setting 作为模型的输入序列\n",
    "        actual_setting_inputs_for_rnn = actual_setting[:-1] # 形状: (seq_len - 1)\n",
    "\n",
    "        # 模型输入序列长度\n",
    "        rnn_input_len = seq_len - 1\n",
    "\n",
    "        # 将整个输入序列一次性输入到模型中，获取所有预测结果\n",
    "        # 这是模拟 RNN 的序列处理，而不是真正的单步循环预测\n",
    "        # 如果想做真正的单步预测，需要循环并手动管理 hidden_state\n",
    "        # 为了可视化对比所有步的预测，一次性输入更方便\n",
    "\n",
    "        # 准备输入张量 (增加批次维)\n",
    "        delta_t_inputs_tensor = torch.tensor(actual_delta_t_inputs_for_rnn, dtype=torch.float32).unsqueeze(0).to(device) # Shape: (1, seq_len - 1)\n",
    "        setting_inputs_tensor = torch.tensor(actual_setting_inputs_for_rnn, dtype=torch.long).unsqueeze(0).to(device) # Shape: (1, seq_len - 1)\n",
    "\n",
    "        # 准备 lengths tensor\n",
    "        lengths_tensor = torch.tensor([rnn_input_len]) # 保持在 CPU\n",
    "\n",
    "        # 将隐藏状态移动到设备\n",
    "        hidden_state = hidden_state.to(device) if hidden_state is not None else None\n",
    "\n",
    "        # 通过 RNN 进行前向传播\n",
    "        # 模型输出的是 predicted_next_delta_t 和 predicted_next_setting_logits\n",
    "        # predicted_next_delta_t 形状: (1, rnn_input_len)\n",
    "        # predicted_next_setting_logits 形状: (1, rnn_input_len, num_categories)\n",
    "        predicted_next_delta_t_seq, predicted_next_setting_logits_seq, _ = model(delta_t_inputs_tensor, setting_inputs_tensor, lengths_tensor, hidden_state)\n",
    "\n",
    "        # 转换为 numpy 数组\n",
    "        predicted_delta_t = predicted_next_delta_t_seq.squeeze(0).cpu().numpy() # Shape: (rnn_input_len,) = (seq_len - 1,)\n",
    "        predicted_setting = torch.argmax(predicted_next_setting_logits_seq.squeeze(0), dim=-1).cpu().numpy() # Shape: (rnn_input_len,) = (seq_len - 1,)\n",
    "\n",
    "        # 计算预测的绝对时间序列 (从 actual_time[1] 开始)\n",
    "        # predicted_time_abs[i] = actual_time[i] + predicted_delta_t[i] (i from 0 to seq_len-2)\n",
    "        predicted_time_abs = actual_time[:-1] + predicted_delta_t # Shape: (seq_len - 1)\n",
    "\n",
    "\n",
    "        # 实际的时间和设置序列需要截取到预测结果的长度一致 (从第二个元素开始)\n",
    "        # 预测序列长度是 seq_len - 1\n",
    "        # 它们预测的是 actual_time[1:] 的绝对时间和 actual_setting[1:] 的设置\n",
    "        actual_time_sliced = actual_time[1:] # 形状: (seq_len - 1)\n",
    "        actual_setting_sliced = actual_setting[1:] # 形状: (seq_len - 1)\n",
    "\n",
    "\n",
    "        return actual_time_sliced, predicted_time_abs, actual_setting_sliced, predicted_setting\n",
    "\n",
    "\n",
    "def visualize_prediction(actual_time, predicted_time, actual_setting, predicted_setting, sample_index, original_label, assigned_cluster_label):\n",
    "    \"\"\"\n",
    "    可视化单个序列的实际值与模型预测值的对比。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 序列 {sample_index} (原始标签: {original_label}, 分配到: {assigned_cluster_label}) 的步进式预测可视化 ---\")\n",
    "\n",
    "    # 获取预测序列的步数\n",
    "    num_steps_predicted = len(predicted_time)\n",
    "    if num_steps_predicted == 0:\n",
    "        print(\"没有可预测的步数来可视化。\")\n",
    "        return\n",
    "    # 创建 x 轴的步数索引 (从 1 开始，对应原始序列的索引 1 到 seq_len - 1)\n",
    "    step_indices = np.arange(1, num_steps_predicted + 1)\n",
    "\n",
    "    # 绘制时间预测对比图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(step_indices, actual_time, label='实际时间', marker='o', linestyle='-', color='blue')\n",
    "    plt.plot(step_indices, predicted_time, label='预测时间', marker='x', linestyle='--', color='red')\n",
    "    plt.title(f'序列 {sample_index} 时间预测对比 (实际 vs 预测)')\n",
    "    plt.xlabel('预测步数 (对应原始序列索引 1 至末尾)')\n",
    "    plt.ylabel('时间')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制设置预测对比图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # combined_setting 的值域是 0-124，直接作为 y 轴值\n",
    "    plt.plot(step_indices, actual_setting, label='实际设置', marker='o', linestyle='-', color='blue')\n",
    "    plt.plot(step_indices, predicted_setting, label='预测设置', marker='x', linestyle='--', color='red')\n",
    "    plt.title(f'序列 {sample_index} 设置预测对比 (实际 vs 预测)')\n",
    "    plt.xlabel('预测步数 (对应原始序列索引 1 至末尾)')\n",
    "    plt.ylabel('Combined Setting (0-124)')\n",
    "    # 可以设置 y 轴刻度，如果需要更精细的展示\n",
    "    # plt.yticks(np.arange(0, 125, 10)) # 示例：每隔 10 个类别显示一个刻度\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "setting_categories = [-2, -1, 0, 1, 2] # 假设这是您的原始 setting categories\n",
    "# 创建一个有序的 CategoricalDtype\n",
    "setting_dtype = pd.CategoricalDtype(categories=setting_categories, ordered=True)\n",
    "\n",
    "def restore_from_transformed_element(transformed_element):\n",
    "    \"\"\"\n",
    "    将一个转换后的数据元素 (来自 transformed_list) 还原为原始格式。\n",
    "\n",
    "    Args:\n",
    "        transformed_element: 列表，一个元素来自 transformed_list，格式为\n",
    "                             [transformed_dataframe, label]。\n",
    "                             transformed_dataframe 包含 'time', 'combined_setting' 列。\n",
    "                             'combined_setting' 列是 dtype='category'，其类别是 0-124 的整数。\n",
    "\n",
    "    Returns:\n",
    "        list: 还原后的元素，格式为 [restored_dataframe, label]。\n",
    "              restored_dataframe 包含 'time', 'top_setting', 'central_setting',\n",
    "              'bottom_setting' 列。这三列是 dtype='category'，其类别是 -2到2。\n",
    "    Raises:\n",
    "        ValueError: 如果输入格式不正确。\n",
    "    \"\"\"\n",
    "    if not isinstance(transformed_element, list) or len(transformed_element) != 2:\n",
    "        raise ValueError(\"Input must be a list of two elements: [dataframe, label]\")\n",
    "\n",
    "    transformed_df, label = transformed_element\n",
    "\n",
    "    if not isinstance(transformed_df, pd.DataFrame):\n",
    "         raise ValueError(\"First element of the input list must be a pandas DataFrame\")\n",
    "\n",
    "    if not all(col in transformed_df.columns for col in ['time', 'combined_setting']):\n",
    "         raise ValueError(\"Input DataFrame must contain 'time' and 'combined_setting' columns\")\n",
    "\n",
    "    # 确保 'combined_setting' 是 category 类型，并且其类别是整数 (0-124)\n",
    "    # 使用 .astype(int) 来获取其内部整数表示\n",
    "    try:\n",
    "        combined_values = transformed_df['combined_setting'].astype(int)\n",
    "    except ValueError as e:\n",
    "         raise ValueError(f\"Could not convert 'combined_setting' to int, check category values: {e}\")\n",
    "\n",
    "\n",
    "    # 逆转五进制编码 (分解)\n",
    "    # combined_setting_id = mapped_top * 25 + mapped_central * 5 + mapped_bottom * 1\n",
    "    # mapped values are 0-4, corresponding to -2 to 2\n",
    "\n",
    "    mapped_top = combined_values // 25         # 整数除法\n",
    "    remainder = combined_values % 25           # 取余数\n",
    "    mapped_central = remainder // 5\n",
    "    mapped_bottom = remainder % 5              # 或者 remainder // 1, 但 % 5 更清晰表示末位\n",
    "\n",
    "    # 逆转映射 (将 0-4 还原到 -2-2)\n",
    "    top_setting_numeric = mapped_top - 2\n",
    "    central_setting_numeric = mapped_central - 2\n",
    "    bottom_setting_numeric = mapped_bottom - 2\n",
    "\n",
    "    # 创建新的 DataFrame，包含原始 time 和还原后的三列设置\n",
    "    restored_df = pd.DataFrame({\n",
    "        'time': transformed_df['time'],\n",
    "        'top_setting': top_setting_numeric,\n",
    "        'central_setting': central_setting_numeric,\n",
    "        'bottom_setting': bottom_setting_numeric\n",
    "    })\n",
    "\n",
    "    # 将还原后的设置列转换为 category 类型，使用之前定义的有序类别和 dtype\n",
    "    for col in ['top_setting', 'central_setting', 'bottom_setting']:\n",
    "        # 确保转换后的数值在 [-2, 2] 范围内，否则 astype(setting_dtype) 可能产生 NaN\n",
    "        # 如果编码正确，这应该是保证的\n",
    "        try:\n",
    "            restored_df[col] = restored_df[col].astype(setting_dtype)\n",
    "        except ValueError as e:\n",
    "             print(f\"警告: 还原列 '{col}' 包含超出类别范围的值，转换为 category 时可能出现 NaN: {e}\")\n",
    "             restored_df[col] = pd.Categorical(restored_df[col], categories=setting_categories, ordered=True) # 即使有超出范围的也尝试转换\n",
    "\n",
    "    # 返回还原后的 [dataframe, label] 元素\n",
    "    return [restored_df, label]\n",
    "\n",
    "def calculate_row_condition_proportion(data_list, condition_func):\n",
    "    \"\"\"\n",
    "    计算一个列表中的所有 DataFrame 合并后，满足特定行条件的比例。\n",
    "\n",
    "    Args:\n",
    "        data_list: 列表，每个元素是 [dataframe, label]。\n",
    "        condition_func: 一个函数，接收一个 DataFrame 的行 (Pandas Series)，返回 True 或 False。\n",
    "\n",
    "    Returns:\n",
    "        float: 满足条件的行数 / 总行数。如果总行数为 0，返回 0.0。\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    for df, _ in data_list:\n",
    "        if df is not None and not df.empty:\n",
    "             # 确保只选择 setting 列\n",
    "             all_rows.append(df[['top_setting', 'central_setting', 'bottom_setting']].astype(int)) # 将 category 转为 int 进行计算\n",
    "\n",
    "    if not all_rows:\n",
    "        return 0.0 # 没有数据框或数据框为空\n",
    "\n",
    "    combined_df = pd.concat(all_rows, ignore_index=True)\n",
    "    total_rows = len(combined_df)\n",
    "\n",
    "    if total_rows == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # 使用 apply along axis 1 to check condition for each row\n",
    "    # condition_func will receive a Series representing a row of setting columns\n",
    "    # pandas apply can be slow, but for clarity let's use it first.\n",
    "    # A vectorized approach is better if possible.\n",
    "\n",
    "    # Vectorized approach for the specific conditions required:\n",
    "\n",
    "    # Condition: exactly one setting is non-zero\n",
    "    if condition_func.__name__ == 'is_one_setting_nonzero':\n",
    "         # Count non-zero settings in each row (0 for non-zero, 1 for zero)\n",
    "         # Summing booleans treats True as 1, False as 0\n",
    "         num_nonzero = (combined_df != 0).sum(axis=1) # axis=1 sums across columns for each row\n",
    "         satisfied_rows = (num_nonzero == 1).sum() # Count rows where num_nonzero is exactly 1\n",
    "\n",
    "    # Condition: any setting is 1 or -1\n",
    "    elif condition_func.__name__ == 'is_any_setting_1_or_neg1':\n",
    "         # Check if any setting in the row is 1 or -1\n",
    "         is_1_or_neg1 = combined_df.isin([1, -1]) # Boolean DataFrame of same shape\n",
    "         satisfied_rows = is_1_or_neg1.any(axis=1).sum() # Check if 'any' is True in each row, then sum\n",
    "\n",
    "    else:\n",
    "         # Fallback to apply for general condition functions (less efficient)\n",
    "         print(f\"警告: 使用通用的 apply 方法计算条件 '{condition_func.__name__}' 的比例，效率较低。\")\n",
    "         satisfied_rows = combined_df.apply(condition_func, axis=1).sum()\n",
    "\n",
    "\n",
    "    return satisfied_rows / total_rows\n",
    "\n",
    "\n",
    "# Helper condition functions for clarity\n",
    "def is_one_setting_nonzero(row):\n",
    "     # row is a Pandas Series of shape (3,) representing ['top', 'central', 'bottom'] settings\n",
    "     # Convert to numpy array for boolean calculation\n",
    "     settings = row.values\n",
    "     # Count how many settings are not equal to 0\n",
    "     num_nonzero = np.sum(settings != 0)\n",
    "     return num_nonzero == 1\n",
    "\n",
    "def is_any_setting_1_or_neg1(row):\n",
    "    # row is a Pandas Series of shape (3,)\n",
    "    settings = row.values\n",
    "    # Check if any setting is equal to 1 or -1\n",
    "    return np.any((settings == 1) | (settings == -1))\n",
    "\n",
    "\n",
    "def calculate_label_proportion(data_list, target_label):\n",
    "    \"\"\"\n",
    "    计算一个列表中元素标签为特定字符串的比例。\n",
    "\n",
    "    Args:\n",
    "        data_list: 列表，每个元素是 [dataframe, label]。\n",
    "        target_label: 目标标签字符串。\n",
    "\n",
    "    Returns:\n",
    "        float: 标签为 target_label 的元素数量 / 列表总元素数量。如果列表为空，返回 0.0。\n",
    "    \"\"\"\n",
    "    total_elements = len(data_list)\n",
    "    if total_elements == 0:\n",
    "        return 0.0\n",
    "\n",
    "    target_label_count = sum(1 for df, label in data_list if label == target_label)\n",
    "\n",
    "    return target_label_count / total_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型运行\n",
    "df, meta = pyreadstat.read_sav(r\"E:\\复旦大学\\大四下\\毕业论文\\代码\\rawdata\\CBA_cp025q01_logs12_SPSS.sav\")\n",
    "result = split_strict_paired_events(df)#取出每一条行为链\n",
    "first_invalid_df = verify_sub_dfs_pattern(result)#检查数据规律\n",
    "transformed_result = transform_sub_dfs(result)#取出所有的apply与reset行\n",
    "clean_result = remove_consecutive_duplicates(transformed_result)#删去相邻重复行\n",
    "final_result_list_raw = create_time_series_with_result(transformed_result)#取出所需数据列，存入列表中\n",
    "#final_result_list = random.sample(final_result_list_raw, 10000)\n",
    "transformed_list_mix = transform_setting_data_handle_categorical_input(final_result_list_raw)\n",
    "transformed_list=[item for item in transformed_list_mix if len(item[0])>=3]#去除过短的序列\n",
    "\n",
    "seeds=0\n",
    "while(1):\n",
    "    seeds +=1000\n",
    "    random.seed(seeds)\n",
    "    np.random.seed(seeds)\n",
    "    torch.manual_seed(seeds)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seeds)\n",
    "        torch.backends.cudnn.deterministic = True # 确保 CUDA 确定性算法\n",
    "    transformed_list=[item for item in transformed_list_mix if len(item[0])>=3]#去除过短的序列\n",
    "    random.shuffle(transformed_list)\n",
    "    TOTAL_SEQUENCES = len(transformed_list) # 更新总序列数常量\n",
    "\n",
    "    print(f\"{seeds}:总共生成 {TOTAL_SEQUENCES} 条模拟序列 ({len([item for item in transformed_list if item[1] == \"rand_label\"])} 条干扰项)。\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # 运行聚类算法\n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\n--- 运行基于 RNN (预测 delta_t) 的聚类算法 (带干扰项处理) ---\")\n",
    "    # 修复：将 high_loss_threshold 参数名改为 high_avg_loss_threshold\n",
    "    final_models, final_assignments, removed_interference_indices_final = run_rnn_clustering(\n",
    "        transformed_list=transformed_list,\n",
    "        num_main_models=NUM_MAIN_MODELS,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        num_rnn_layers=NUM_RNN_LAYERS,\n",
    "        num_categories=NUM_COMBINED_SETTINGS,\n",
    "        time_scaler=TIME_LOSS_SCALER, # 注意：这里可能需要根据实际 delta_t 值的范围调整 scaler\n",
    "        total_iterations=TOTAL_EM_ITERATIONS, # 使用更新后的迭代次数\n",
    "        convergence_threshold=CONVERGENCE_THRESHOLD,\n",
    "        epoch_schedule=EPOCH_SCHEDULE, # 使用更新后的 epoch 计划表\n",
    "        batch_size=BATCH_SIZE,\n",
    "        early_iter_batch_threshold=EARLY_ITER_BATCH_THRESHOLD, # 使用更新后的阈值\n",
    "        early_iter_batch_percent=EARLY_ITER_BATCH_PERCENT,\n",
    "        interference_cluster_label=INTERFERENCE_CLUSTER_LABEL, # 干扰项簇标签\n",
    "        interference_detection_start_iter=INTERFERENCE_DETECTION_START_ITER, # 干扰项检测起始迭代\n",
    "        high_avg_loss_threshold=HIGH_AVG_LOSS_THRESHOLD, # 高平均损失阈值\n",
    "        num_rand_sequences=NUM_RAND_SEQUENCES # 已知的干扰项数量 (用于选出最高损失的 N 个)\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # 聚类结果可视化 (包含干扰项类别)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    visualize_clustering_results(transformed_list, final_assignments, NUM_MAIN_MODELS, INTERFERENCE_CLUSTER_LABEL)\n",
    "    # 初始化四个空列表，对应簇 0, 1, 2 和 3 (干扰项)\n",
    "    # 这些列表将存储还原后的 [dataframe, label] 元素\n",
    "    cluster_1 = [] # 对应簇 0\n",
    "    cluster_2 = [] # 对应簇 1\n",
    "    cluster_3 = [] # 对应簇 2\n",
    "    trival_cluster = [] # 对应簇 3 (干扰项)\n",
    "\n",
    "    # 创建一个字典，将簇索引映射到对应的列表\n",
    "    cluster_map = {\n",
    "        0: cluster_1,\n",
    "        1: cluster_2,\n",
    "        2: cluster_3,\n",
    "        INTERFERENCE_CLUSTER_LABEL: trival_cluster # 使用常量\n",
    "    }\n",
    "\n",
    "    # 遍历 transformed_list 的索引\n",
    "    for i in tqdm(range(len(transformed_list)), desc=\"分配并还原序列\"):\n",
    "        # 获取当前元素的分配结果\n",
    "        assignment = final_assignments[i]\n",
    "\n",
    "        # 获取当前元素的原始 transformed 格式\n",
    "        transformed_element = transformed_list[i]\n",
    "\n",
    "        # 只有分配结果在 cluster_map 的键中才进行处理\n",
    "        if assignment in cluster_map:\n",
    "            # 调用 restore_from_transformed_element 函数进行还原\n",
    "\n",
    "            restored_element = restore_from_transformed_element(transformed_element)\n",
    "            # 将还原后的元素添加到对应的列表中\n",
    "            cluster_map[assignment].append(restored_element)\n",
    "        else:\n",
    "            # 处理未预期的分配值 (例如 -1, 如果聚类算法返回了未分配的序列)\n",
    "            print(f\"警告: 序列 {i} 具有未预期的分配值: {assignment}。原始标签: {transformed_element[1]}。还原后未分配。\")\n",
    "            # 这些序列不会被添加到任何 cluster_X 或 trival_cluster 列表中\n",
    "\n",
    "    INTERFERENCE_LABEL_CHECK = 'NULL'\n",
    "    TARGET_LABEL_FOR_CHECK_2_3 = \"'100101\"\n",
    "\n",
    "    print(\"--- 检查聚类条件 ---\")\n",
    "\n",
    "    # 将簇列表存储在一个方便访问的字典中\n",
    "    clusters = {\n",
    "        'cluster_1': cluster_1,\n",
    "        'cluster_2': cluster_2,\n",
    "        'cluster_3': cluster_3,\n",
    "        'trival_cluster': trival_cluster\n",
    "    }\n",
    "\n",
    "    # 检查条件 1: trival_cluster 中标签为 INTERFERENCE_LABEL_CHECK 的比例最高\n",
    "    print(f\"\\n检查条件 1: trival_cluster 中标签为 '{INTERFERENCE_LABEL_CHECK}' 的比例是否最高\")\n",
    "    label_proportions_interference = {}\n",
    "    for name, data_list in clusters.items():\n",
    "        proportion = calculate_label_proportion(data_list, INTERFERENCE_LABEL_CHECK)\n",
    "        proportion_ = calculate_label_proportion(data_list, r\"'000000\")\n",
    "        label_proportions_interference[name] = proportion\n",
    "        print(f\"  {name} 中标签为 '{INTERFERENCE_LABEL_CHECK}' 的比例: {proportion:.4f}\")\n",
    "\n",
    "    # 找到比例最高的簇\n",
    "    max_prop_interference_cluster = max(label_proportions_interference, key=label_proportions_interference.get)\n",
    "    max_prop_interference_value = label_proportions_interference[max_prop_interference_cluster]\n",
    "\n",
    "    # 检查 trival_cluster 是否是比例最高的，且其比例严格大于其他簇\n",
    "    # 需要遍历所有其他簇，确保 trival_cluster 的比例都严格大于它们\n",
    "    is_trival_highest = True\n",
    "    trival_prop = label_proportions_interference.get('trival_cluster', 0.0) # 如果trival_cluster为空，get返回0\n",
    "\n",
    "    for name, prop in label_proportions_interference.items():\n",
    "        if name != 'trival_cluster':\n",
    "            if trival_prop <= prop: # 如果 trival_cluster 的比例不严格大于或等于其他簇的比例\n",
    "                is_trival_highest = False\n",
    "                break\n",
    "\n",
    "    Bool1 = is_trival_highest\n",
    "    print(f\"条件 1 结果: trival_cluster ({trival_prop:.4f}) 中标签 '{INTERFERENCE_LABEL_CHECK}' 的比例是否严格高于其他簇: {Bool1}\")\n",
    "    # 检查条件 2: 在 cluster_1,2,3 中，setting列只有一个不为0的比例最高的簇也是标签 TARGET_LABEL_FOR_CHECK_2_3 比例最高的簇\n",
    "    print(f\"\\n检查条件 2: 主簇中 '一个 setting 不为 0' 比例最高的簇是否也对应标签 '{TARGET_LABEL_FOR_CHECK_2_3}' 比例最高\")\n",
    "    main_clusters = {\n",
    "        'cluster_1': cluster_1,\n",
    "        'cluster_2': cluster_2,\n",
    "        'cluster_3': cluster_3\n",
    "    }\n",
    "\n",
    "    one_nonzero_setting_proportions = {}\n",
    "    target_label_proportions_main = {}\n",
    "    for name, data_list in main_clusters.items():\n",
    "        # 计算 '一个 setting 不为 0' 的行比例\n",
    "        prop_one_nonzero = calculate_row_condition_proportion(data_list, is_one_setting_nonzero)\n",
    "        one_nonzero_setting_proportions[name] = prop_one_nonzero\n",
    "        print(f\"  {name} 中 '一个 setting 不为 0' 的行比例: {prop_one_nonzero:.4f}\")\n",
    "\n",
    "        # 计算标签为 TARGET_LABEL_FOR_CHECK_2_3 的元素比例\n",
    "        prop_target_label = calculate_label_proportion(data_list, TARGET_LABEL_FOR_CHECK_2_3)\n",
    "        target_label_proportions_main[name] = prop_target_label\n",
    "        print(f\"  {name} 中标签为 '{TARGET_LABEL_FOR_CHECK_2_3}' 的元素比例: {prop_target_label:.4f}\")\n",
    "\n",
    "    # 找到 '一个 setting 不为 0' 比例最高的簇\n",
    "    # 如果所有比例都为 0，max 会返回第一个键。这里我们假设在有意义的数据上运行。\n",
    "    if one_nonzero_setting_proportions: # 避免在 main_clusters 都为空时出错\n",
    "        max_one_nonzero_cluster = max(one_nonzero_setting_proportions, key=one_nonzero_setting_proportions.get)\n",
    "        # 找到标签 TARGET_LABEL_FOR_CHECK_2_3 比例最高的簇\n",
    "        max_target_label_main_cluster = max(target_label_proportions_main, key=target_label_proportions_main.get)\n",
    "\n",
    "        # 检查两个簇是否是同一个\n",
    "        Bool2 = (max_one_nonzero_cluster == max_target_label_main_cluster)\n",
    "        print(f\"条件 2 结果: '一个 setting 不为 0' 比例最高的簇是 {max_one_nonzero_cluster}，标签 '{TARGET_LABEL_FOR_CHECK_2_3}' 比例最高的簇是 {max_target_label_main_cluster}。它们是否一致: {Bool2}\")\n",
    "    else:\n",
    "        Bool2 = False\n",
    "        print(\"条件 2 结果: 主簇列表为空，无法检查条件 2。 Bool2 = False\")\n",
    "\n",
    "\n",
    "    # 检查条件 3: 所有簇中 setting 列取 1 或 -1 的比例最高的簇，其标签为 TARGET_LABEL_FOR_CHECK_2_3 的比例最低\n",
    "    print(f\"\\n检查条件 3: 'setting 为 1 或 -1' 比例最高的簇是否对应标签 '{TARGET_LABEL_FOR_CHECK_2_3}' 比例最低\")\n",
    "\n",
    "    setting_1_or_neg1_proportions = {}\n",
    "    target_label_proportions_all = {} # 计算所有四个簇中标签 TARGET_LABEL_FOR_CHECK_2_3 的比例\n",
    "\n",
    "    for name, data_list in clusters.items():\n",
    "        # 计算 'setting 为 1 或 -1' 的行比例\n",
    "        prop_1_or_neg1 = calculate_row_condition_proportion(data_list, is_any_setting_1_or_neg1)\n",
    "        setting_1_or_neg1_proportions[name] = prop_1_or_neg1\n",
    "        print(f\"  {name} 中 'setting 为 1 或 -1' 的行比例: {prop_1_or_neg1:.4f}\")\n",
    "\n",
    "        # 计算标签为 TARGET_LABEL_FOR_CHECK_2_3 的元素比例 (包括 trival_cluster)\n",
    "        prop_target_label_all = calculate_label_proportion(data_list, TARGET_LABEL_FOR_CHECK_2_3)\n",
    "        target_label_proportions_all[name] = prop_target_label_all\n",
    "        print(f\"  {name} 中标签为 '{TARGET_LABEL_FOR_CHECK_2_3}' 的元素比例: {prop_target_label_all:.4f}\")\n",
    "\n",
    "\n",
    "    # 找到 'setting 为 1 或 -1' 比例最高的簇 (在所有四个簇中)\n",
    "    if setting_1_or_neg1_proportions: # 避免所有簇都为空时出错\n",
    "        max_1_or_neg1_cluster = max(setting_1_or_neg1_proportions, key=setting_1_or_neg1_proportions.get)\n",
    "\n",
    "        # 找到标签 TARGET_LABEL_FOR_CHECK_2_3 比例最低的簇 (在所有四个簇中)\n",
    "        # 确保在非空列表中查找最小值\n",
    "        if target_label_proportions_all:\n",
    "            min_target_label_all_cluster = min(target_label_proportions_all, key=target_label_proportions_all.get)\n",
    "\n",
    "            # 检查两个簇是否是同一个\n",
    "            Bool3 = (max_1_or_neg1_cluster == min_target_label_all_cluster)\n",
    "            print(f\"条件 3 结果: 'setting 为 1 或 -1' 比例最高的簇是 {max_1_or_neg1_cluster}，标签 '{TARGET_LABEL_FOR_CHECK_2_3}' 比例最低的簇是 {min_target_label_all_cluster}。它们是否一致: {Bool3}\")\n",
    "        else:\n",
    "            Bool3 = False\n",
    "            print(\"条件 3 结果: 标签比例列表为空，无法检查条件 3。 Bool3 = False\")\n",
    "\n",
    "    else:\n",
    "        Bool3 = False\n",
    "        print(\"条件 3 结果: 'setting 为 1 或 -1' 比例列表为空，无法检查条件 3。 Bool3 = False\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "    # 输出最终的 boolean 结果\n",
    "    # ----------------------------------------------------------------------------\n",
    "    print(\"\\n--- 最终检查结果 ---\")\n",
    "    print(f\"Bool1 (trival_cluster 中 '{INTERFERENCE_LABEL_CHECK}' 比例是否最高): {Bool1}\")\n",
    "    print(f\"Bool2 (主簇中 '一个 setting 不为 0' 比例最高的簇是否也对应标签 '{TARGET_LABEL_FOR_CHECK_2_3}' 比例最高): {Bool2}\")\n",
    "    print(f\"Bool3 (所有簇中 'setting 为 1 或 -1' 比例最高的簇是否对应标签 '{TARGET_LABEL_FOR_CHECK_2_3}' 比例最低): {Bool3}\")\n",
    "\n",
    "    if(Bool1 and Bool2):\n",
    "        print(\"seed={}\".format(seeds))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c7f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_1[10]\n",
    "cluster_sizes = {\n",
    "    '簇 0': len(cluster_1),\n",
    "    '簇 1': len(cluster_2),\n",
    "    '簇 2': len(cluster_3),\n",
    "    '干扰项': len(trival_cluster)\n",
    "}\n",
    "blue_colors = ['lightblue', 'skyblue', 'steelblue', 'cornflowerblue', 'dodgerblue']\n",
    "labels = list(cluster_sizes.keys())\n",
    "sizes = list(cluster_sizes.values())\n",
    "non_empty_labels = [labels[i] for i in range(len(labels)) if sizes[i] > 0]\n",
    "non_empty_sizes = [sizes[i] for i in range(len(labels)) if sizes[i] > 0]\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(non_empty_sizes, labels=non_empty_labels, autopct='%1.1f%%', startangle=0 ,colors=blue_colors,textprops={'fontsize': 12})\n",
    "\n",
    "# 添加标题\n",
    "plt.title('聚类结果分布比例', fontsize=16)\n",
    "\n",
    "# 等比例绘制，使饼图为圆形\n",
    "plt.axis('equal')\n",
    "\n",
    "# 显示图例 (可选，如果标签已经显示在饼图上)\n",
    "# plt.legend(labels, loc=\"best\")\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "# 定义 setting 的可能取值 (数值形式)\n",
    "setting_values_numeric = [-2, -1, 0, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3afd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 辅助函数: 将 (top, central, bottom) 元组编码为整数 ID ---\n",
    "def encode_setting_tuple(top, central, bottom):\n",
    "    \"\"\"将 setting tuple 编码为 0-124 的整数 ID。\"\"\"\n",
    "    # 将 -2,-1,0,1,2 映射到 0,1,2,3,4\n",
    "    mapped_top = top + 2\n",
    "    mapped_central = central + 2\n",
    "    mapped_bottom = bottom + 2\n",
    "    # 应用五进制编码\n",
    "    return mapped_top * 25 + mapped_central * 5 + mapped_bottom\n",
    "\n",
    "# --- 辅助函数: 将整数 ID 解码回 (top, central, bottom) 元组 ---\n",
    "def decode_setting_id(encoded_id):\n",
    "    \"\"\"将 0-124 的整数 ID 解码回 setting tuple。\"\"\"\n",
    "    if not 0 <= encoded_id <= 124:\n",
    "        raise ValueError(f\"Encoded ID out of range: {encoded_id}\")\n",
    "    # 逆转五进制编码\n",
    "    mapped_top = encoded_id // 25\n",
    "    remainder = encoded_id % 25\n",
    "    mapped_central = remainder // 5\n",
    "    mapped_bottom = remainder % 5\n",
    "    # 逆转映射 (0-4 回到 -2-2)\n",
    "    top = mapped_top - 2\n",
    "    central = mapped_central - 2\n",
    "    bottom = mapped_bottom - 2\n",
    "    return (top, central, bottom)\n",
    "\n",
    "\n",
    "# --- 辅助函数: 合并列表中的所有 DataFrame 的设置列并编码 ---\n",
    "def combine_and_encode_setting_dataframes(data_list):\n",
    "    \"\"\"\n",
    "    合并一个列表中的所有 DataFrame 的设置列，将每行设置编码为整数 ID，\n",
    "    返回包含 encoded_setting_id 的单个 DataFrame。\n",
    "\n",
    "    Args:\n",
    "        data_list: 列表，每个元素是 [dataframe, label]。\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame: 包含 'encoded_setting_id' 列。如果列表为空或数据框为空，返回空 DataFrame。\n",
    "    \"\"\"\n",
    "    all_setting_dfs = []\n",
    "    for df, _ in data_list:\n",
    "        if df is not None and not df.empty and all(col in df.columns for col in ['top_setting', 'central_setting', 'bottom_setting']):\n",
    "             # 选择设置列并转换为整数，方便编码\n",
    "             setting_df_numeric = df[['top_setting', 'central_setting', 'bottom_setting']].astype(int)\n",
    "             # 对每行应用编码函数，结果将是 Series 或 array of encoded IDs\n",
    "             # 使用 apply(axis=1) 或向量化方法 (如果设置值是整数)\n",
    "             # 向量化方法更高效\n",
    "             mapped_top = setting_df_numeric['top_setting'] + 2\n",
    "             mapped_central = setting_df_numeric['central_setting'] + 2\n",
    "             mapped_bottom = setting_df_numeric['bottom_setting'] + 2\n",
    "             encoded_ids = mapped_top * 25 + mapped_central * 5 + mapped_bottom\n",
    "\n",
    "             all_setting_dfs.append(pd.DataFrame({'encoded_setting_id': encoded_ids}))\n",
    "\n",
    "\n",
    "    if not all_setting_dfs:\n",
    "        # 返回一个空的 DataFrame，但带有正确的列名\n",
    "        return pd.DataFrame(columns=['encoded_setting_id'])\n",
    "\n",
    "    # 合并所有编码后的 DataFrame\n",
    "    combined_encoded_df = pd.concat(all_setting_dfs, ignore_index=True)\n",
    "    return combined_encoded_df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. 准备数据并计算总体设置组合分布 (用于确定表格的纵轴)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"--- 分析主簇中 setting 组合的分布 ---\")\n",
    "\n",
    "# 将主簇列表存储在一个方便访问的字典中\n",
    "main_clusters = {\n",
    "    'cluster_1': cluster_1,\n",
    "    'cluster_2': cluster_2,\n",
    "    'cluster_3': cluster_3\n",
    "}\n",
    "\n",
    "# 合并并编码所有主簇的设置列数据框\n",
    "all_main_clusters_encoded_df = combine_and_encode_setting_dataframes(cluster_1 + cluster_2 + cluster_3)\n",
    "\n",
    "# 计算在所有主簇合并后的数据中，每个编码 ID 的总出现次数\n",
    "# value_counts() 默认按频率降序排列\n",
    "if not all_main_clusters_encoded_df.empty:\n",
    "    encoded_id_counts_overall = all_main_clusters_encoded_df['encoded_setting_id'].value_counts()\n",
    "else:\n",
    "    encoded_id_counts_overall = pd.Series(dtype=int) # 空 Series\n",
    "\n",
    "# 选择出现次数最高的前 10 个编码 ID\n",
    "# 如果总组合数少于 10，则取所有存在的组合\n",
    "top_10_encoded_ids = encoded_id_counts_overall.head(10).index.tolist()\n",
    "\n",
    "# 将前 10 个编码 ID 解码回 (top, central, bottom) 元组，作为表格的纵轴标签\n",
    "top_10_setting_combinations = [decode_setting_id(int(id)) for id in top_10_encoded_ids]\n",
    "# 将元组标签转换为更易读的字符串格式\n",
    "top_10_setting_labels = [str(comb) for comb in top_10_setting_combinations]\n",
    "\n",
    "\n",
    "print(f\"\\n总体占比最高的 {len(top_10_encoded_ids)} 种 setting 组合 (用于表格纵轴):\")\n",
    "for i, label in enumerate(top_10_setting_labels):\n",
    "    print(f\"  {label} (编码ID: {top_10_encoded_ids[i]})\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. 构建表格数据 (计算每个主簇中选定 setting 组合的占比)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# 创建一个空的 DataFrame 用于存储表格数据\n",
    "# 索引是选定的 setting 组合标签，列是主簇名称\n",
    "setting_combination_distribution_table = pd.DataFrame(index=top_10_setting_labels, columns=main_clusters.keys())\n",
    "\n",
    "# 遍历每个主簇，计算其中选定 setting 组合的占比\n",
    "for cluster_name, data_list in main_clusters.items():\n",
    "    cluster_encoded_df = combine_and_encode_setting_dataframes(data_list)\n",
    "    total_rows_in_cluster = len(cluster_encoded_df)\n",
    "\n",
    "    if total_rows_in_cluster == 0:\n",
    "        print(f\"警告: 簇 {cluster_name} 为空，其占比均为 0。\")\n",
    "        # 如果簇为空，则该簇所有选定组合的占比都为 0\n",
    "        setting_combination_distribution_table[cluster_name] = 0.0\n",
    "        continue\n",
    "\n",
    "    # 遍历选定的前 10 个编码 ID\n",
    "    for i, encoded_id in enumerate(top_10_encoded_ids):\n",
    "        # 计算当前簇中该编码 ID 出现的次数\n",
    "        # 使用 value_counts() 或直接过滤并计数\n",
    "        count_in_cluster = (cluster_encoded_df['encoded_setting_id'] == encoded_id).sum()\n",
    "\n",
    "        # 计算占比\n",
    "        proportion_in_cluster = count_in_cluster / total_rows_in_cluster\n",
    "\n",
    "        # 将占比填入表格对应的位置\n",
    "        combination_label = top_10_setting_labels[i]\n",
    "        setting_combination_distribution_table.loc[combination_label, cluster_name] = proportion_in_cluster\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. 绘制表格 (使用热力图可视化)\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n绘制主簇中 setting 组合分布表格 (热力图)...\")\n",
    "\n",
    "if setting_combination_distribution_table.empty:\n",
    "    print(\"没有数据可绘制表格。请检查主簇是否包含数据。\")\n",
    "else:\n",
    "    # 调整图大小，使其更容易阅读\n",
    "    plt.figure(figsize=(10, max(6, len(top_10_setting_labels) * 0.7)))\n",
    "    sns.heatmap(setting_combination_distribution_table.astype(float), annot=True, fmt=\".3f\", cmap=\"Blues\", linewidths=.5) # 格式化为小数点后三位\n",
    "    plt.title(\"主簇中 Setting 组合分布占比 (前 10 组合)\")\n",
    "    plt.xlabel(\"主簇\")\n",
    "    plt.ylabel(\"Setting 组合 (Top, Central, Bottom)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"表格绘制完毕。\")\n",
    "setting_combination_distribution_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置中文字体，解决可视化乱码问题 (与之前的设置保持一致)\n",
    "plt.rcParams['font.family'] = ['SimHei'] # 使用黑体，或其他支持中文的字体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像时负号'-'显示为方块的问题\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 假设以下变量已经由之前的分配还原步骤生成并可用:\n",
    "# cluster_1: 列表，包含簇 0 的还原后序列 [restored_dataframe, label]\n",
    "# cluster_2: 列表，包含簇 1 的还原后序列 [restored_dataframe, label]\n",
    "# cluster_3: 列表，包含簇 2 的还原后序列 [restored_dataframe, label]\n",
    "# trival_cluster: 列表，包含干扰项簇 (簇 3) 的还原后序列 [restored_dataframe, label]\n",
    "#\n",
    "# restored_dataframe 包含列: \"time\", \"top_setting\", \"central_setting\", \"bottom_setting\"\n",
    "# 这些 setting 列的 dtype 是 category，类别是 -2, -1, 0, 1, 2。\n",
    "# label 是原始的字符串标签。\n",
    "#\n",
    "# 您需要确保这些列表在运行此代码时已经加载。\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# 定义 setting 的可能取值 (数值形式)\n",
    "setting_values_numeric = [-2, -1, 0, 1, 2]\n",
    "\n",
    "# --- 辅助函数: 将 (top, central, bottom) 元组编码为整数 ID ---\n",
    "def encode_setting_tuple(top, central, bottom):\n",
    "    \"\"\"将 setting tuple 编码为 0-124 的整数 ID。\"\"\"\n",
    "    # 将 -2,-1,0,1,2 映射到 0,1,2,3,4\n",
    "    mapped_top = top + 2\n",
    "    mapped_central = central + 2\n",
    "    mapped_bottom = bottom + 2\n",
    "    # 应用五进制编码\n",
    "    return mapped_top * 25 + mapped_central * 5 + mapped_bottom\n",
    "\n",
    "# --- 辅助函数: 计算单个 DataFrame 中的连续点击次数 ---\n",
    "def count_consecutive_clicks(dataframe):\n",
    "    \"\"\"\n",
    "    计算一个 DataFrame 中设置组合连续出现 3 行或以上的次数。\n",
    "    每当一个连续运行达到或超过 3 行时，算作一次连续点击事件。\n",
    "    一个长度为 N >= 3 的连续运行只算作 1 次事件。\n",
    "\n",
    "    Args:\n",
    "        dataframe: 单个样本的 DataFrame，包含 'top_setting', 'central_setting', 'bottom_setting' 列。\n",
    "\n",
    "    Returns:\n",
    "        int: 该样本中的连续点击事件次数。\n",
    "    \"\"\"\n",
    "    if dataframe is None or dataframe.empty:\n",
    "        return 0\n",
    "\n",
    "    # 确保有足够的行来形成连续 3 行\n",
    "    if len(dataframe) < 3:\n",
    "        return 0\n",
    "\n",
    "    # 选择设置列并转换为整数\n",
    "    setting_df_numeric = dataframe[['top_setting', 'central_setting', 'bottom_setting']].astype(int)\n",
    "\n",
    "    # 对每行应用编码函数，得到 encoded IDs 序列\n",
    "    # 使用向量化方法进行编码\n",
    "    mapped_top = setting_df_numeric['top_setting'] + 2\n",
    "    mapped_central = setting_df_numeric['central_setting'] + 2\n",
    "    mapped_bottom = setting_df_numeric['bottom_setting'] + 2\n",
    "    encoded_ids = mapped_top * 25 + mapped_central * 5 + mapped_bottom\n",
    "\n",
    "    # 使用 pandas Series 来利用 .groupby() 方法\n",
    "    encoded_series = pd.Series(encoded_ids)\n",
    "\n",
    "    # Groupby consecutive identical values\n",
    "    # (encoded_series != encoded_series.shift()) creates a boolean Series that is True\n",
    "    # at the start of each run of identical values.\n",
    "    # .cumsum() creates a unique group ID for each run.\n",
    "    groups = encoded_series.groupby((encoded_series != encoded_series.shift()).cumsum())\n",
    "\n",
    "    num_consecutive_runs = 0\n",
    "    # Iterate through the groups (each group is a consecutive run of identical values)\n",
    "    for group_id, group in groups:\n",
    "        # If the run length is 3 or more, count it as one consecutive click event\n",
    "        if len(group) >= 3:\n",
    "            num_consecutive_runs += 1\n",
    "\n",
    "    return num_consecutive_runs\n",
    "\n",
    "# --- 辅助函数: 计算列表中样本的平均连续点击次数 ---\n",
    "def calculate_average_consecutive_clicks(data_list):\n",
    "    \"\"\"\n",
    "    计算一个列表中所有样本的平均连续点击次数。\n",
    "\n",
    "    Args:\n",
    "        data_list: 列表，每个元素是 [dataframe, label]。\n",
    "\n",
    "    Returns:\n",
    "        float: 平均连续点击次数。如果列表为空，返回 0.0。\n",
    "    \"\"\"\n",
    "    total_samples = len(data_list)\n",
    "    if total_samples == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_clicks_sum = 0\n",
    "    for df, _ in data_list:\n",
    "        clicks_in_sample = count_consecutive_clicks(df)\n",
    "        total_clicks_sum += clicks_in_sample\n",
    "\n",
    "    # 计算平均值\n",
    "    average_clicks = total_clicks_sum / total_samples\n",
    "\n",
    "    return average_clicks\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. 计算每个主簇的平均连续点击次数\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"--- 计算每个主簇的平均连续点击次数 ---\")\n",
    "\n",
    "# 将主簇列表存储在一个方便访问的字典中\n",
    "main_clusters = {\n",
    "    'cluster_1': cluster_1,\n",
    "    'cluster_2': cluster_2,\n",
    "    'cluster_3': cluster_3\n",
    "}\n",
    "\n",
    "# 计算每个主簇的平均连续点击次数\n",
    "average_clicks_per_cluster = {}\n",
    "for name, data_list in main_clusters.items():\n",
    "    average_clicks = calculate_average_consecutive_clicks(data_list)\n",
    "    average_clicks_per_cluster[name] = average_clicks\n",
    "    print(f\"  {name} 的平均连续点击次数: {average_clicks:.4f}\")\n",
    "\n",
    "\n",
    "# 准备绘制条形图的数据\n",
    "cluster_names = list(average_clicks_per_cluster.keys())\n",
    "avg_clicks = list(average_clicks_per_cluster.values())\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. 绘制条形图\n",
    "# ----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n绘制主簇平均连续点击次数条形图...\")\n",
    "\n",
    "plt.figure(figsize=(8, 6)) # 设置图的大小\n",
    "\n",
    "# 绘制条形图\n",
    "x_positions = np.arange(len(cluster_names))\n",
    "plt.bar(x_positions, avg_clicks, color=blue_colors)\n",
    "\n",
    "# 设置 x 轴标签\n",
    "plt.xticks(x_positions, cluster_names)\n",
    "\n",
    "# 设置 y 轴标签和标题\n",
    "plt.ylabel(\"平均连续点击次数\")\n",
    "plt.title(\"各主簇平均连续点击次数\")\n",
    "\n",
    "# 在每个条形上方显示数值\n",
    "for i, avg in enumerate(avg_clicks):\n",
    "    plt.text(i, avg + (max(avg_clicks) * 0.05 if avg_clicks else 0.01), f'{avg:.3f}', ha='center', fontsize=10) # 稍微抬高文本，避免与条形顶部重叠\n",
    "\n",
    "# 设置 y 轴起始为 0 (对于计数通常从 0 开始比较合理)\n",
    "plt.ylim(0, max(avg_clicks) * 1.2 if avg_clicks else 0.1) # 确保 y 轴上限合理\n",
    "\n",
    "# 添加横向网格线\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 紧凑布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n",
    "print(\"条形图绘制完毕。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
