{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62e71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacleanv2 import *\n",
    "from SetRNN import *\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from collections import Counter # 用于统计计数的工具\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import time # 用于计时\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils # 用于处理变长序列，如填充和打包\n",
    "from torch.utils.data import Dataset, DataLoader # PyTorch 数据加载工具\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # 进度条库，使用 tqdm.tqdm\n",
    "import random\n",
    "import copy # 用于复制模型参数或列表\n",
    "import matplotlib.pyplot as plt # 用于绘图\n",
    "import seaborn as sns # 用于更美观的统计图，特别是热力图\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EARLY_ITER_BATCH_THRESHOLD = 3 # 在前 3 轮迭代中使用部分批次 (适应总迭代 10)\n",
    "EARLY_ITER_BATCH_PERCENT = 0.3\n",
    "\n",
    "# 超参数和常量定义\n",
    "NUM_MAIN_MODELS = 3 # 主要的聚类模型数量\n",
    "NUM_COMBINED_SETTINGS = 23 # combined_setting 的总类别数 (0-124)\n",
    "EMBEDDING_DIM = 8 # combined_setting 的嵌入向量维度，可调整\n",
    "HIDDEN_SIZE = 64   # RNN 隐藏层大小，可调整\n",
    "NUM_RNN_LAYERS = 2 # RNN 层数\n",
    "# 注意: TIME_LOSS_SCALER 可能需要根据实际 delta_t 的规模重新调整\n",
    "TIME_LOSS_SCALER = 1 # time delta_t MSE 损失的缩放因子，需要根据实际损失值大小调整\n",
    "TOTAL_EM_ITERATIONS = 10 # EM 迭代总次数 (根据要求修改为 10)\n",
    "CONVERGENCE_THRESHOLD = 0.05 # 收敛阈值，分配改变的序列比例低于此值时停止 (5%)\n",
    "\n",
    "# 干扰项处理参数\n",
    "NUM_RAND_SEQUENCES = 250 # 干扰项的已知数量\n",
    "INTERFERENCE_CLUSTER_LABEL = 3 # 将干扰项分配到的簇的索引 (0, 1, 2 是主簇，3 是干扰簇)\n",
    "INTERFERENCE_DETECTION_START_ITER = 2 # 从第 5 轮迭代 (索引 4) 的 E 步开始检测干扰项\n",
    "# 检测干扰项的高损失阈值：需要根据训练中观察到的损失值范围来调整\n",
    "# 如果一个序列在所有模型上的平均损失超过这个阈值，则可能被认为是干扰项。\n",
    "# ！！！重要参数，需要根据实际运行观察的损失值调整！！！\n",
    "# 在模拟数据上运行一次，观察损失值的分布，尤其是 rand_label 序列的损失。\n",
    "HIGH_AVG_LOSS_THRESHOLD = 0.5 ## <--- !!! 初始值，请务必根据实际情况调整 !!!\n",
    "\n",
    "# M 步训练参数 (每个 EM 迭代中的训练 epochs)\n",
    "# epochs 计划表：根据迭代次数使用不同数量的 epochs\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32 # M 步训练时的批次大小\n",
    "# 在早期迭代中是否只使用部分批次来加速训练\n",
    "EARLY_ITER_BATCH_THRESHOLD = 3 # 在前 3 轮迭代中使用部分批次 (适应总迭代 10)\n",
    "EARLY_ITER_BATCH_PERCENT = 0.3 # 在启用部分批次训练时使用的批次比例 (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110a945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = pd.read_csv(r\"E:\\复旦大学\\研一上\\科研\\评分剪枝算法\\数据\\traffic\\traffic_q2_response.csv\")\n",
    "df_seq = pd.read_csv(r\"E:\\复旦大学\\研一上\\科研\\评分剪枝算法\\数据\\traffic\\traffic_q2_seq.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffd2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切割后得到 31656 个数据框\n"
     ]
    }
   ],
   "source": [
    "# 1. 将action的列名改为combined_setting\n",
    "df_seq = df_seq.rename(columns={'action': 'combined_setting'})\n",
    "df_seq[\"combined_setting\"] = df_seq[\"combined_setting\"] - 1  # 假设原始数据是从1开始的，这里将其转换为从0开始\n",
    "# 2. 将数据框切割，连续的ID相同的行被切割进同一个数据框\n",
    "# 创建分组标识：当ID发生变化时开始新分组\n",
    "\n",
    "# 按分组切割数据框并组成列表\n",
    "grouped_dfs = [group for _, group in df_seq.groupby('ID')]\n",
    "\n",
    "print(f\"切割后得到 {len(grouped_dfs)} 个数据框\")\n",
    "\n",
    "# 3. 去除每个数据框元素的第一行和最后一行\n",
    "# 4. 去除所有长度小于3的数据框（去除后长度至少为1）\n",
    "filtered_dfs = []\n",
    "\n",
    "for i, df_group in enumerate(grouped_dfs):\n",
    "    # 如果数据框长度大于等于3，才进行处理\n",
    "    if len(df_group) >= 5:\n",
    "        # 去除第一行和最后一行\n",
    "        trimmed_df = df_group.iloc[1:-1].copy()\n",
    "      \n",
    "        matching_row = df_response[df_response['ID'] == trimmed_df['ID'].iloc[0]]\n",
    "        response_value = matching_row['Response'].iloc[0] \n",
    "        trimmed_df = trimmed_df.drop(columns=['ID'])\n",
    "        filtered_dfs.append([trimmed_df, response_value])  # 假设 df_response 的索引与 grouped_dfs 对应\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c4bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_list = filtered_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb549f0f",
   "metadata": {},
   "source": [
    "### 对整体的所有数据训练出一个大RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13190a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, time_criterion, setting_criterion, epochs, time_scaler, iteration_num,writer):\n",
    "    model.train() # 设置模型为训练模式\n",
    "    total_batches = len(dataloader)\n",
    "    # 根据迭代次数决定使用的批次数量\n",
    "    if iteration_num < EARLY_ITER_BATCH_THRESHOLD:\n",
    "        batches_to_use = max(1, int(total_batches * EARLY_ITER_BATCH_PERCENT))\n",
    "    else:\n",
    "        batches_to_use = total_batches\n",
    "\n",
    "    # 使用 tqdm 显示 epoch 进度，使用传入的 model_idx\n",
    "    epoch_tqdm = tqdm(range(epochs), desc=f\"迭代 {iteration_num} (模型 ) 训练\", leave=False)\n",
    "    for epoch in epoch_tqdm:\n",
    "        total_epoch_loss = 0\n",
    "        total_time_loss = 0\n",
    "        total_setting_loss = 0\n",
    "        batch_count = 0\n",
    "        # 使用 tqdm 显示批次进度,dataloader每次迭代返回一个batch（4条序列）和一条长度表\n",
    "        batch_tqdm = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch_data in batch_tqdm:\n",
    "            if batch_data is None: continue # 跳过空批次\n",
    "\n",
    "            # 从 collate_fn 获取批次数据,填充和对t作差分是在collate_fn中完成,将setting嵌入是在rnn中完成\n",
    "            delta_t_inputs, setting_inputs, delta_t_targets, setting_targets, lengths = batch_data\n",
    "\n",
    "            optimizer.zero_grad() # 清零梯度\n",
    "\n",
    "            # 前向传播\n",
    "            # 模型现在输出的是 predicted_next_delta_t 和 predicted_next_setting_logits\n",
    "            predicted_next_delta_t, predicted_next_setting_logits, _ = model(delta_t_inputs, setting_inputs, lengths)\n",
    "\n",
    "            # 计算损失\n",
    "            # predicted_next_delta_t 形状: (batch_size, seq_len)\n",
    "            # delta_t_targets 形状: (batch_size, seq_len)\n",
    "            time_loss = time_criterion(predicted_next_delta_t, delta_t_targets)\n",
    "\n",
    "            # predicted_next_setting_logits 形状: (batch_size, seq_len, num_categories)\n",
    "            # setting_targets 形状: (batch_size, seq_len)\n",
    "            # 需要调整 logits 的维度到 (batch_size, num_categories, seq_len)\n",
    "            setting_loss = setting_criterion(predicted_next_setting_logits.permute(0, 2, 1), setting_targets)\n",
    "\n",
    "            # 计算总损失，并应用时间损失缩放因子\n",
    "            loss = time_loss * time_scaler + setting_loss\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            # 可选：梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss += loss.item() # 计算epoch累计损失\n",
    "            total_time_loss += time_loss.item()\n",
    "            total_setting_loss += setting_loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            # 在早期迭代中，只训练部分批次\n",
    "            if batch_count >= batches_to_use:\n",
    "                 break\n",
    "\n",
    "            # 更新批次进度条的后缀信息（可选）\n",
    "            batch_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        final_epoch_avg_loss = total_epoch_loss / batch_count \n",
    "        final_epoch_avg_time_loss = total_time_loss / batch_count\n",
    "        final_epoch_avg_setting_loss = total_setting_loss / batch_count\n",
    "        writer.add_scalar(\"Loss/Total_train\",final_epoch_avg_loss,epoch+1) # 记录每个 epoch 的平均损失到 TensorBoard\n",
    "        writer.add_scalar('Loss/Time_Loss_Train', final_epoch_avg_time_loss, epoch)\n",
    "        writer.add_scalar('Loss/Setting_Loss_Train', final_epoch_avg_setting_loss, epoch)\n",
    "        \n",
    "        # 更新 epoch 进度条的后缀信息（可选）\n",
    "        # 注意：这里显示的是最后一个批次的损失，不是平均损失\n",
    "        epoch_tqdm.set_postfix(last_batch_loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4412426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sequence_loss(model, time_seq_full, setting_seq_full, time_criterion, setting_criterion, time_scaler):\n",
    "    model.eval() # 设置模型为评估模式\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "\n",
    "        seq_len = time_seq_full.size(0)\n",
    "        # 序列长度小于 3 无法构建输入和目标序列 (长度 original_length - 2)\n",
    "        if seq_len < 3:\n",
    "            # 返回无穷大，表示无法计算有效损失，在比较时会被排除\n",
    "            return float('inf')\n",
    "\n",
    "        # --- 构建输入序列 (当前 delta_t 和 setting) 和目标序列 (下一个 delta_t 和 setting) ---\n",
    "        # 它们都对应原始序列长度 - 2 的部分\n",
    "\n",
    "        # delta_t 输入: time[i+1] - time[i] for i from 0 to seq_len - 3\n",
    "        delta_t_inputs_sliced = (time_seq_full[1:-1] - time_seq_full[:-2]).unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "        # setting 输入: setting[i] for i from 0 to seq_len - 3\n",
    "        setting_inputs_sliced = setting_seq_full[:-2].unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "\n",
    "        # delta_t 目标: time[i+2] - time[i+1] for i from 0 to seq_len - 3\n",
    "        delta_t_targets_sliced = (time_seq_full[2:] - time_seq_full[1:-1]).unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "        # setting 目标: setting[i+1] for i from 0 to seq_len - 3\n",
    "        setting_targets_sliced = setting_seq_full[1:-1].unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "\n",
    "        # 输入序列的实际长度\n",
    "        eval_input_len = seq_len - 2 # 有效的预测步数\n",
    "        lengths = torch.tensor([eval_input_len]) # 保持在 CPU\n",
    "\n",
    "        # 前向传播，计算整个序列的预测结果 (长度为 eval_input_len)\n",
    "        predicted_next_delta_t, predicted_next_setting_logits, _ = model(delta_t_inputs_sliced, setting_inputs_sliced, lengths)\n",
    "\n",
    "        # 计算整个序列的损失 (注意：损失函数如 MSELoss 和 CrossEntropyLoss 默认计算的是批次和序列长度上的平均)\n",
    "        # 但在这里我们处理的是单个序列 (batch_size = 1)，并且使用了 pack_padded_sequence，\n",
    "        # PyTorch 会确保损失计算只在有效长度上进行。\n",
    "        # 所以 time_criterion(predicted_next_delta_t, delta_t_targets_sliced) 计算的是 batch 和 有效长度上的平均损失。\n",
    "        # setting_criterion(...) 也类似。\n",
    "\n",
    "        time_loss = time_criterion(predicted_next_delta_t, delta_t_targets_sliced)\n",
    "        setting_loss = setting_criterion(predicted_next_setting_logits.permute(0, 2, 1), setting_targets_sliced)\n",
    "\n",
    "        # 计算总损失 (按步加权平均)\n",
    "        total_loss_per_step = time_loss * time_scaler + setting_loss\n",
    "\n",
    "        # total_loss_per_step 现在已经是每个预测步的平均损失了 (因为 MSELoss/CrossEntropyLoss 默认对 batch 和序列长度取平均)\n",
    "        # 但是 pack_padded_sequence 的行为可能会影响这个平均，为了安全和明确，我们还是按总损失再除以步数\n",
    "        # 更稳妥的方法是使用 reduction='sum' 然后手动除以有效步数\n",
    "        time_criterion_sum = nn.MSELoss(reduction='sum')\n",
    "        setting_criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        time_loss_sum = time_criterion_sum(predicted_next_delta_t, delta_t_targets_sliced)\n",
    "        setting_loss_sum = setting_criterion_sum(predicted_next_setting_logits.permute(0, 2, 1), setting_targets_sliced)\n",
    "\n",
    "        total_sum_loss = time_loss_sum * time_scaler + setting_loss_sum\n",
    "    \n",
    "    \n",
    "        # 计算 **平均** 损失：总和损失除以有效预测步数\n",
    "        average_loss_per_step = total_sum_loss / eval_input_len\n",
    "        average_set_loss_per_step = setting_loss_sum / eval_input_len\n",
    "\n",
    "\n",
    "        # 返回标量平均损失值\n",
    "        return average_loss_per_step.item(),average_set_loss_per_step.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1260dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn_optimize(transformed_list,  embedding_dim, hidden_size, num_rnn_layers,\n",
    "                       num_categories, time_scaler, total_iterations=10, \n",
    "                       epochs=20,batch_size=32):\n",
    "    # 使用当前时间创建唯一的日志目录，以区分不同的训练运行\n",
    "    log_dir = f\"runs/rnn_training_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"TensorBoard 日志将保存在: {log_dir}\")\n",
    "          \n",
    "    total_sequences = len(transformed_list)\n",
    "    print(f\"开始基于 RNN 的聚类，共有 {total_sequences} 条序列，聚成一类\")\n",
    "\n",
    "    # 1. 初始化\n",
    "    # 创建一个RNN 模型\n",
    "    model = SettingPredictorRNN(embedding_dim, hidden_size, num_rnn_layers, num_categories).to(device) \n",
    "    #所有序列均长度大于3\n",
    "\n",
    "    # 定义损失函数 (用于 E 步计算单序列损失，使用 reduction='sum')\n",
    "    time_criterion_sum = nn.MSELoss(reduction='sum')\n",
    "    setting_criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "    # 定义损失函数 (用于 M 步训练批次，使用默认 reduction='mean')\n",
    "    time_criterion_mean = nn.MSELoss()\n",
    "    setting_criterion_mean = nn.CrossEntropyLoss()\n",
    "\n",
    "    # active_indices: 获取当前仍在参与聚类的主簇序列的原始索引 (即 current_assignments != interference_cluster_label 的序列)\n",
    "    active_indices_mask = np.array([len(item[0]) for item in transformed_list]) >= 3 \n",
    "    active_indices = range(len(transformed_list))\n",
    "    current_total_active_sequences = len(active_indices)\n",
    "\n",
    "    assigned_indices_in_active = active_indices\n",
    "\n",
    "  \n",
    "    assigned_dataset = SequenceDataset([transformed_list[i][0] for i in assigned_indices_in_active])\n",
    "    # collate_fn 将过滤掉长度不足 3 的序列，DataLoader 会处理批次和填充\n",
    "    assigned_dataloader = DataLoader(assigned_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    # 只有当 DataLoader 不为空时才训练 (即存在长度 >= 3 的序列)\n",
    "    if len(assigned_dataloader) > 0:\n",
    "        # 为当前模型创建一个优化器\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # 调用训练函数训练当前模型，传入模型索引，使用 mean reduction 的损失函数\n",
    "        train_model(model,assigned_dataloader, optimizer,\n",
    "                time_criterion_mean, setting_criterion_mean, epochs,\n",
    "                time_scaler, total_iterations,writer)\n",
    "        \n",
    "    writer.close() # 关闭 TensorBoard 日志记录器\n",
    "    return model # 返回 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca7b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 日志将保存在: runs/rnn_training_20250827-121203\n",
      "开始基于 RNN 的聚类，共有 26773 条序列，聚成一类\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aff33e9e1c74e25b571918588d0e017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "迭代 10 (模型 ) 训练:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c531a43eab547958e836e343dbe3cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9593a655c25941f9a3625ce77529a78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da257d70b1044d47981520bed2bf824c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0289fce6591e4521b449e3b092438e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6fe1841cec34a829229d9cd9449219d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882634cccac74435b6e8add433cccdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcafaee32b374a679c126d8b305d2bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1058d2531ff43e19df7caff079dd92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5026ce1c45d549f6a6ecf1598cacdd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0770358dceb4505a95d4799c36cf0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65375ff310c442f1a1b29326d9aa25c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a25268747024183b7cdcd55f9e82fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed93ad0eb7941b08d37dae0af58c113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271b9c0ce62e4d0299db9e804ccd8b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1153ff7708445a5a4e0f45dd53cd1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4468ea3df16c47e28921192daba18a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f051b9b4ee04f9d963076413a973558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61cb5bfb0c045909be881e5e1bcad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3765d3531c1c4c02b93c489090aef7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4545f83445544a10b38e042bb4081ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model = run_rnn_optimize(transformed_list,  EMBEDDING_DIM, HIDDEN_SIZE, NUM_RNN_LAYERS,\n",
    "                       NUM_COMBINED_SETTINGS, TIME_LOSS_SCALER, TOTAL_EM_ITERATIONS,EPOCHS,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bd02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model,\"TotalModel_traffic.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
