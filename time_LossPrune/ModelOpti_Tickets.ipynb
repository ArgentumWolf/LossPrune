{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243297c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacleanv2 import *\n",
    "from SetRNN import *\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    "from collections import Counter # 用于统计计数的工具\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import time # 用于计时\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils # 用于处理变长序列，如填充和打包\n",
    "from torch.utils.data import Dataset, DataLoader # PyTorch 数据加载工具\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # 进度条库，使用 tqdm.tqdm\n",
    "import random\n",
    "import copy # 用于复制模型参数或列表\n",
    "import matplotlib.pyplot as plt # 用于绘图\n",
    "import seaborn as sns # 用于更美观的统计图，特别是热力图\n",
    "import pickle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EARLY_ITER_BATCH_THRESHOLD = 3 # 在前 3 轮迭代中使用部分批次 (适应总迭代 10)\n",
    "EARLY_ITER_BATCH_PERCENT = 0.3\n",
    "\n",
    "# 超参数和常量定义\n",
    "NUM_MAIN_MODELS = 3 # 主要的聚类模型数量\n",
    "NUM_COMBINED_SETTINGS = 13 # combined_setting 的总类别数 (0-124)\n",
    "EMBEDDING_DIM = 8 # combined_setting 的嵌入向量维度，可调整\n",
    "HIDDEN_SIZE = 64   # RNN 隐藏层大小，可调整\n",
    "NUM_RNN_LAYERS = 2 # RNN 层数\n",
    "# 注意: TIME_LOSS_SCALER 可能需要根据实际 delta_t 的规模重新调整\n",
    "TIME_LOSS_SCALER = 1# time delta_t MSE 损失的缩放因子，需要根据实际损失值大小调整\n",
    "SETTING_LOSS_SCALER = 0\n",
    "TOTAL_EM_ITERATIONS = 10 # EM 迭代总次数 (根据要求修改为 10)\n",
    "CONVERGENCE_THRESHOLD = 0.05 # 收敛阈值，分配改变的序列比例低于此值时停止 (5%)\n",
    "\n",
    "# 干扰项处理参数\n",
    "NUM_RAND_SEQUENCES = 250 # 干扰项的已知数量\n",
    "INTERFERENCE_CLUSTER_LABEL = 3 # 将干扰项分配到的簇的索引 (0, 1, 2 是主簇，3 是干扰簇)\n",
    "INTERFERENCE_DETECTION_START_ITER = 2 # 从第 5 轮迭代 (索引 4) 的 E 步开始检测干扰项\n",
    "# 检测干扰项的高损失阈值：需要根据训练中观察到的损失值范围来调整\n",
    "# 如果一个序列在所有模型上的平均损失超过这个阈值，则可能被认为是干扰项。\n",
    "# ！！！重要参数，需要根据实际运行观察的损失值调整！！！\n",
    "# 在模拟数据上运行一次，观察损失值的分布，尤其是 rand_label 序列的损失。\n",
    "HIGH_AVG_LOSS_THRESHOLD = 0.5 ## <--- !!! 初始值，请务必根据实际情况调整 !!!\n",
    "\n",
    "# M 步训练参数 (每个 EM 迭代中的训练 epochs)\n",
    "# epochs 计划表：根据迭代次数使用不同数量的 epochs\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32 # M 步训练时的批次大小\n",
    "# 在早期迭代中是否只使用部分批次来加速训练\n",
    "EARLY_ITER_BATCH_THRESHOLD = 3 # 在前 3 轮迭代中使用部分批次 (适应总迭代 10)\n",
    "EARLY_ITER_BATCH_PERCENT = 0.3 # 在启用部分批次训练时使用的批次比例 (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faa65a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_with_result(result: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    将子 DataFrame 列表中的每个元素转换为一个包含时间序列结果的列表。\n",
    "\n",
    "    Args:\n",
    "        result: 包含 Pandas DataFrame 的列表。\n",
    "\n",
    "    Returns:\n",
    "        一个最终列表，其中每个元素都是一个包含时间序列 的DataFrame的列表。\n",
    "    \"\"\"\n",
    "    final_list = []\n",
    "    for df in result:\n",
    "        if not df.empty:\n",
    "            # 1. 提取时间序列数据\n",
    "            time_series_df = df[['time', 'event_value']].copy()\n",
    "            time_series_df = time_series_df.rename(columns = {\"event_value\":\"combined_setting\"})\n",
    "            final_list.append(time_series_df)\n",
    "        else:\n",
    "            final_list.append(pd.DataFrame(columns=['time','combined_setting'])) # 处理空 DataFrame\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def encode_event_values(dataframes_list: list[pd.DataFrame]) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    将每个dataframe中的combined_setting列从字符串映射为0-12的数字编码\n",
    "    \n",
    "    Args:\n",
    "        dataframes_list: 包含多个dataframe的列表，每个dataframe有time和combined_setting两列\n",
    "        \n",
    "    Returns:\n",
    "        转换后的dataframe列表，combined_setting列变为数字编码\n",
    "    \"\"\"\n",
    "    # 定义映射字典\n",
    "    event_mapping = {\n",
    "        'Buy': 0,\n",
    "        'Cancel': 1,\n",
    "        'city_subway': 2,\n",
    "        'concession': 3,\n",
    "        'country_trains': 4,\n",
    "        'daily': 5,\n",
    "        'full_fare': 6,\n",
    "        'individual': 7,\n",
    "        'trip_1': 8,\n",
    "        'trip_2': 9,\n",
    "        'trip_3': 10,\n",
    "        'trip_4': 11,\n",
    "        'trip_5': 12\n",
    "    }\n",
    "\n",
    "    # 处理每个dataframe\n",
    "    encoded_dataframes = []\n",
    "    \n",
    "    for i, df in enumerate(dataframes_list):\n",
    "        # 检查dataframe结构\n",
    "        if not {'time', 'combined_setting'}.issubset(df.columns):\n",
    "            raise ValueError(f\"第{i}个dataframe缺少time或combined_setting列\")\n",
    "        # 复制dataframe以避免修改原始数据\n",
    "        df_encoded = df.copy()\n",
    "        # 映射combined_setting列\n",
    "        df_encoded['combined_setting'] = df_encoded['combined_setting'].map(event_mapping)\n",
    "        encoded_dataframes.append(df_encoded)\n",
    "    \n",
    "    return encoded_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff16e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,mata = pyreadstat.read_sav(r\"E:\\复旦大学\\研一上\\科研\\评分剪枝算法\\数据\\tickets\\CBA_cp038q01_logs12_SPSS.sav\")\n",
    "result =[item[1:-1] for item in split_strict_paired_events(df)]\n",
    "final_result_list_raw=create_time_series_with_result(result)\n",
    "transformed_list = encode_event_values(final_result_list_raw)\n",
    "filtered_list = [[df,''] for df in transformed_list if len(df)>=3]\n",
    "#加一个''作为response占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a13ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_list=filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cbd30",
   "metadata": {},
   "source": [
    "### 对整体的所有数据训练出一个大RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10644ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, time_criterion, setting_criterion, epochs, time_scaler, setting_scaler,iteration_num,writer):\n",
    "    model.train() # 设置模型为训练模式\n",
    "    total_batches = len(dataloader)\n",
    "    # 根据迭代次数决定使用的批次数量\n",
    "    if iteration_num < EARLY_ITER_BATCH_THRESHOLD:\n",
    "        batches_to_use = max(1, int(total_batches * EARLY_ITER_BATCH_PERCENT))\n",
    "    else:\n",
    "        batches_to_use = total_batches\n",
    "\n",
    "    # 使用 tqdm 显示 epoch 进度，使用传入的 model_idx\n",
    "    epoch_tqdm = tqdm(range(epochs), desc=f\"迭代 {iteration_num} (模型 ) 训练\", leave=False)\n",
    "    for epoch in epoch_tqdm:\n",
    "        total_epoch_loss = 0\n",
    "        total_time_loss = 0\n",
    "        total_setting_loss = 0\n",
    "        batch_count = 0\n",
    "        # 使用 tqdm 显示批次进度,dataloader每次迭代返回一个batch（4条序列）和一条长度表\n",
    "        batch_tqdm = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch_data in batch_tqdm:\n",
    "            if batch_data is None: continue # 跳过空批次\n",
    "\n",
    "            # 从 collate_fn 获取批次数据,填充和对t作差分是在collate_fn中完成,将setting嵌入是在rnn中完成\n",
    "            delta_t_inputs, setting_inputs, delta_t_targets, setting_targets, lengths = batch_data\n",
    "\n",
    "            optimizer.zero_grad() # 清零梯度\n",
    "\n",
    "            # 前向传播\n",
    "            # 模型现在输出的是 predicted_next_delta_t 和 predicted_next_setting_logits\n",
    "            predicted_next_delta_t, predicted_next_setting_logits, _ = model(delta_t_inputs, setting_inputs, lengths)\n",
    "\n",
    "            # 计算损失\n",
    "            # predicted_next_delta_t 形状: (batch_size, seq_len)\n",
    "            # delta_t_targets 形状: (batch_size, seq_len)\n",
    "            time_loss = time_criterion(predicted_next_delta_t, delta_t_targets)\n",
    "\n",
    "            # predicted_next_setting_logits 形状: (batch_size, seq_len, num_categories)\n",
    "            # setting_targets 形状: (batch_size, seq_len)\n",
    "            # 需要调整 logits 的维度到 (batch_size, num_categories, seq_len)\n",
    "            setting_loss = setting_criterion(predicted_next_setting_logits.permute(0, 2, 1), setting_targets)\n",
    "\n",
    "            #***************************************************\n",
    "            # ********计算总损失，并应用时间损失缩放因子*******\n",
    "            #***************************************************\n",
    "            loss = time_loss * time_scaler + setting_loss *setting_scaler\n",
    "\n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            # 可选：梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_epoch_loss += loss.item() # 计算epoch累计损失\n",
    "            total_time_loss += time_loss.item()\n",
    "            total_setting_loss += setting_loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            # 在早期迭代中，只训练部分批次\n",
    "            if batch_count >= batches_to_use:\n",
    "                 break\n",
    "\n",
    "            # 更新批次进度条的后缀信息（可选）\n",
    "            batch_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        final_epoch_avg_loss = total_epoch_loss / batch_count \n",
    "        final_epoch_avg_time_loss = total_time_loss / batch_count\n",
    "        final_epoch_avg_setting_loss = total_setting_loss / batch_count\n",
    "        writer.add_scalar(\"Loss/Total_train\",final_epoch_avg_loss,epoch+1) # 记录每个 epoch 的平均损失到 TensorBoard\n",
    "        writer.add_scalar('Loss/Time_Loss_Train', final_epoch_avg_time_loss, epoch)\n",
    "        writer.add_scalar('Loss/Setting_Loss_Train', final_epoch_avg_setting_loss, epoch)\n",
    "        \n",
    "        # 更新 epoch 进度条的后缀信息（可选）\n",
    "        # 注意：这里显示的是最后一个批次的损失，不是平均损失\n",
    "        epoch_tqdm.set_postfix(last_batch_loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec26d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sequence_loss(model, time_seq_full, setting_seq_full, time_criterion, setting_criterion, time_scaler,setting_scaler):\n",
    "    model.eval() # 设置模型为评估模式\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "\n",
    "        seq_len = time_seq_full.size(0)\n",
    "        # 序列长度小于 3 无法构建输入和目标序列 (长度 original_length - 2)\n",
    "        if seq_len < 3:\n",
    "            # 返回无穷大，表示无法计算有效损失，在比较时会被排除\n",
    "            return float('inf')\n",
    "\n",
    "        # --- 构建输入序列 (当前 delta_t 和 setting) 和目标序列 (下一个 delta_t 和 setting) ---\n",
    "        # 它们都对应原始序列长度 - 2 的部分\n",
    "\n",
    "        # delta_t 输入: time[i+1] - time[i] for i from 0 to seq_len - 3\n",
    "        delta_t_inputs_sliced = (time_seq_full[1:-1] - time_seq_full[:-2]).unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "        # setting 输入: setting[i] for i from 0 to seq_len - 3\n",
    "        setting_inputs_sliced = setting_seq_full[:-2].unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "\n",
    "        # delta_t 目标: time[i+2] - time[i+1] for i from 0 to seq_len - 3\n",
    "        delta_t_targets_sliced = (time_seq_full[2:] - time_seq_full[1:-1]).unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "        # setting 目标: setting[i+1] for i from 0 to seq_len - 3\n",
    "        setting_targets_sliced = setting_seq_full[1:-1].unsqueeze(0).to(device) # 形状: (1, seq_len - 2)\n",
    "\n",
    "        # 输入序列的实际长度\n",
    "        eval_input_len = seq_len - 2 # 有效的预测步数\n",
    "        lengths = torch.tensor([eval_input_len]) # 保持在 CPU\n",
    "\n",
    "        # 前向传播，计算整个序列的预测结果 (长度为 eval_input_len)\n",
    "        predicted_next_delta_t, predicted_next_setting_logits, _ = model(delta_t_inputs_sliced, setting_inputs_sliced, lengths)\n",
    "\n",
    "        # 计算整个序列的损失 (注意：损失函数如 MSELoss 和 CrossEntropyLoss 默认计算的是批次和序列长度上的平均)\n",
    "        # 但在这里我们处理的是单个序列 (batch_size = 1)，并且使用了 pack_padded_sequence，\n",
    "        # PyTorch 会确保损失计算只在有效长度上进行。\n",
    "        # 所以 time_criterion(predicted_next_delta_t, delta_t_targets_sliced) 计算的是 batch 和 有效长度上的平均损失。\n",
    "        # setting_criterion(...) 也类似。\n",
    "\n",
    "        time_loss = time_criterion(predicted_next_delta_t, delta_t_targets_sliced)\n",
    "        setting_loss = setting_criterion(predicted_next_setting_logits.permute(0, 2, 1), setting_targets_sliced)\n",
    "\n",
    "        # ****计算总损失 (按步加权平均)*****\n",
    "        total_loss_per_step = time_loss * time_scaler + setting_loss *setting_scaler\n",
    "\n",
    "        # total_loss_per_step 现在已经是每个预测步的平均损失了 (因为 MSELoss/CrossEntropyLoss 默认对 batch 和序列长度取平均)\n",
    "        # 但是 pack_padded_sequence 的行为可能会影响这个平均，为了安全和明确，我们还是按总损失再除以步数\n",
    "        # 更稳妥的方法是使用 reduction='sum' 然后手动除以有效步数\n",
    "        time_criterion_sum = nn.MSELoss(reduction='sum')\n",
    "        setting_criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "        time_loss_sum = time_criterion_sum(predicted_next_delta_t, delta_t_targets_sliced)\n",
    "        setting_loss_sum = setting_criterion_sum(predicted_next_setting_logits.permute(0, 2, 1), setting_targets_sliced)\n",
    "\n",
    "        total_sum_loss = time_loss_sum * time_scaler + setting_loss_sum *setting_scaler\n",
    "    \n",
    "    \n",
    "        # 计算 **平均** 损失：总和损失除以有效预测步数\n",
    "        average_loss_per_step = total_sum_loss / eval_input_len\n",
    "        average_set_loss_per_step = setting_loss_sum / eval_input_len\n",
    "\n",
    "\n",
    "        # 返回标量平均损失值\n",
    "        return average_loss_per_step.item(),average_set_loss_per_step.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d934215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn_optimize(transformed_list,  embedding_dim, hidden_size, num_rnn_layers,\n",
    "                       num_categories, time_scaler, setting_scaler, total_iterations, \n",
    "                       epochs,batch_size):\n",
    "    # 使用当前时间创建唯一的日志目录，以区分不同的训练运行\n",
    "    log_dir = f\"runs/rnn_training_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"TensorBoard 日志将保存在: {log_dir}\")\n",
    "          \n",
    "    total_sequences = len(transformed_list)\n",
    "    print(f\"开始基于 RNN 的聚类，共有 {total_sequences} 条序列，聚成一类\")\n",
    "\n",
    "    # 1. 初始化\n",
    "    # 创建一个RNN 模型\n",
    "    model = SettingPredictorRNN(embedding_dim, hidden_size, num_rnn_layers, num_categories).to(device) \n",
    "    #所有序列均长度大于3\n",
    "\n",
    "    # 定义损失函数 (用于 E 步计算单序列损失，使用 reduction='sum')\n",
    "    time_criterion_sum = nn.MSELoss(reduction='sum')\n",
    "    setting_criterion_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "    # 定义损失函数 (用于 M 步训练批次，使用默认 reduction='mean')\n",
    "    time_criterion_mean = nn.MSELoss()\n",
    "    setting_criterion_mean = nn.CrossEntropyLoss()\n",
    "\n",
    "    # active_indices: 获取当前仍在参与聚类的主簇序列的原始索引 (即 current_assignments != interference_cluster_label 的序列)\n",
    "    active_indices_mask = np.array([len(item[0]) for item in transformed_list]) >= 3 \n",
    "    active_indices = range(len(transformed_list))\n",
    "    current_total_active_sequences = len(active_indices)\n",
    "\n",
    "    assigned_indices_in_active = active_indices\n",
    "\n",
    "  \n",
    "    assigned_dataset = SequenceDataset([transformed_list[i][0] for i in assigned_indices_in_active])\n",
    "    # collate_fn 将过滤掉长度不足 3 的序列，DataLoader 会处理批次和填充\n",
    "    assigned_dataloader = DataLoader(assigned_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    # 只有当 DataLoader 不为空时才训练 (即存在长度 >= 3 的序列)\n",
    "    if len(assigned_dataloader) > 0:\n",
    "        # 为当前模型创建一个优化器\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        # 调用训练函数训练当前模型，传入模型索引，使用 mean reduction 的损失函数\n",
    "        train_model(model,assigned_dataloader, optimizer,\n",
    "                time_criterion_mean, setting_criterion_mean, epochs,\n",
    "                time_scaler, setting_scaler, total_iterations,writer)\n",
    "        \n",
    "    writer.close() # 关闭 TensorBoard 日志记录器\n",
    "    return model # 返回 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42dc4b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 日志将保存在: runs/rnn_training_20251115-012514\n",
      "开始基于 RNN 的聚类，共有 31322 条序列，聚成一类\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfc84bc057a48d480253bdd18c48798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "迭代 10 (模型 ) 训练:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add8868629cb4e5b934bdc259d706de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded79a0fbe82424e8e9beba78440a0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fdf2a45d3741639b7d9983858678bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b868c2263242a69094aeabb71fddc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c44d74687ba4aafaa4d920992804ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913d96e525ae4f91bec20021edc82032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4babf525a88426d8921387b24c87c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109ed78c94ce4e249ea33deec8c482f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaae32021d394d85aefd04d06c3d68b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca605453b1c4cf9ab2d8b0588f86308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba88517161ea4fd69c5efaf2ba65d9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe4818f8056435d84b8f438d7bf40d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b73aff98514dfca5b85c6c8e3d4805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4560e894854c20acbd926d8d99ca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7361cea9ca99400d846468407a641126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc738aa472d54358a3d322339d370523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bc3a7f7fda402995db4cd840d3dd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4e30f7b42e41c493f59081d43d2691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76f91139515454ea0b0401e56c4ab23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3407518b9b4744a177aa51312a9793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_model = run_rnn_optimize(transformed_list,  EMBEDDING_DIM, HIDDEN_SIZE, NUM_RNN_LAYERS,\n",
    "                       NUM_COMBINED_SETTINGS, TIME_LOSS_SCALER, SETTING_LOSS_SCALER,TOTAL_EM_ITERATIONS,EPOCHS,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6729690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model,\"TotalModel_ticket.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
